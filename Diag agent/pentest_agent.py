#!/usr/bin/env python3
"""
╔═══════════════════════════════════════════════════════════════════════════════╗
║              AUTONOMOUS PENTEST AGENT v4.0 - Enhanced Edition                 ║
║                      Master's Security Scanner                                ║
╠═══════════════════════════════════════════════════════════════════════════════╣
║  Features:                                                                    ║
║  • Interactive CLI with guided configuration                                  ║
║  • Multi-threaded scanning for better performance                             ║
║  • SQL Injection detection (error-based + time-based blind)                   ║
║  • XSS detection (reflected)                                                  ║
║  • Command Injection detection                                                ║
║  • Path Traversal / LFI detection                                             ║
║  • CORS Misconfiguration analysis                                             ║
║  • Cookie Security analysis                                                   ║
║  • Technology Fingerprinting                                                  ║
║  • Security header analysis                                                   ║
║  • Sensitive file exposure checks                                             ║
║  • Automated crawling with depth control                                      ║
║  • Multiple report formats (JSONL, JSON, TXT, HTML)                           ║
╚═══════════════════════════════════════════════════════════════════════════════╝

IMPORTANT: Only use on systems you own or have written authorization to test!

Author: [Vitaliy Domin]
Project - Cybersecurity
"""

import argparse
import datetime as dt
import hashlib
import json
import logging
import os
import re
import random
import sys
import time
import urllib3
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, asdict, field
from enum import Enum
from queue import Queue
from threading import Lock
from typing import Dict, List, Optional, Set, Tuple, Any
from urllib.parse import urlparse, urlunparse, urlencode, parse_qsl, urljoin

import requests

# Suppress InsecureRequestWarning for lab environments
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Suppress connection pool warnings (normal during multi-threading)
logging.getLogger("urllib3.connectionpool").setLevel(logging.ERROR)

# Optional: Better HTML parsing
try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False

# ==============================================================================
# COLORS FOR TERMINAL OUTPUT
# ==============================================================================

class Colors:
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'
    END = '\033[0m'
    
    @classmethod
    def disable(cls):
        """Disable colors for non-TTY environments"""
        cls.HEADER = cls.BLUE = cls.CYAN = cls.GREEN = ''
        cls.YELLOW = cls.RED = cls.BOLD = cls.UNDERLINE = cls.END = ''


# Check if output is a TTY
if not sys.stdout.isatty():
    Colors.disable()


# ==============================================================================
# BANNER & UI
# ==============================================================================

BANNER = f"""
{Colors.CYAN}╔═══════════════════════════════════════════════════════════════════════════════╗
║{Colors.BOLD}{Colors.GREEN}              AUTONOMOUS PENTEST AGENT v4.0 - Enhanced Edition                 {Colors.END}{Colors.CYAN}║
║                              Security Scanner                                 ║
╠═══════════════════════════════════════════════════════════════════════════════╣
║  {Colors.YELLOW}[!] WARNING: Only use on systems you have authorization to test!{Colors.CYAN}             ║
╚═══════════════════════════════════════════════════════════════════════════════╝{Colors.END}
"""

MENU = f"""
{Colors.BOLD}{Colors.HEADER}═══════════════════════════════════════════════════════════════════════════════
                              MAIN MENU
═══════════════════════════════════════════════════════════════════════════════{Colors.END}

{Colors.GREEN}[1]{Colors.END} Quick Scan          {Colors.CYAN}→ Fast scan with default settings (recommended for beginners){Colors.END}
{Colors.GREEN}[2]{Colors.END} Full Scan           {Colors.CYAN}→ Complete scan with all modules enabled{Colors.END}
{Colors.GREEN}[3]{Colors.END} Custom Scan         {Colors.CYAN}→ Choose specific modules and configure options{Colors.END}
{Colors.GREEN}[4]{Colors.END} SQLi Only           {Colors.CYAN}→ Focus only on SQL Injection vulnerabilities{Colors.END}
{Colors.GREEN}[5]{Colors.END} XSS Only            {Colors.CYAN}→ Focus only on Cross-Site Scripting vulnerabilities{Colors.END}
{Colors.GREEN}[6]{Colors.END} Headers Only        {Colors.CYAN}→ Check only security headers configuration{Colors.END}
{Colors.GREEN}[7]{Colors.END} File Exposure Only  {Colors.CYAN}→ Check only for sensitive file exposure{Colors.END}
{Colors.GREEN}[8]{Colors.END} Google Dorking Only {Colors.CYAN}→ OSINT reconnaissance using advanced Google search{Colors.END}
{Colors.GREEN}[9]{Colors.END} Command Line Mode   {Colors.CYAN}→ Use traditional CLI arguments (for scripts){Colors.END}
{Colors.GREEN}[0]{Colors.END} Exit

{Colors.BOLD}Enter your choice:{Colors.END} """


# ==============================================================================
# CONFIGURATION & CONSTANTS
# ==============================================================================

DEFAULT_LOG_FILE = "pentest_results.jsonl"
DEFAULT_MAX_REQUESTS = 2000  # Increased from 500
DEFAULT_THREADS = 10         # Number of concurrent threads
DEFAULT_TIMEOUT = 15         # Increased timeout
DEFAULT_DELAY = 0.1          # Reduced delay for faster scanning


class Severity(Enum):
    CRITICAL = "CRITICAL"
    HIGH = "HIGH"
    MEDIUM = "MEDIUM"
    LOW = "LOW"
    INFO = "INFO"


class Confidence(Enum):
    CONFIRMED = "CONFIRMED"
    HIGH = "HIGH"
    MEDIUM = "MEDIUM"
    LOW = "LOW"


class FindingStatus(Enum):
    CONFIRMED = "CONFIRMED"
    ACCESS_CONTROLLED = "ACCESS_CONTROLLED"
    REDIRECTED = "REDIRECTED"
    SPA_ROUTED = "SPA_ROUTED"
    INFORMATIONAL = "INFORMATIONAL"
    ENUMERATED = "ENUMERATED"
    NOT_FOUND = "NOT_FOUND"
    NOT_EXPOSED = "NOT_EXPOSED"


class ExplanationDatabase:
    """Provides human-readable, auditor-friendly explanations for findings."""
    
    EXPLANATIONS = {
        "SQL Injection": {
            "text": "The application appears to be vulnerable to SQL Injection, where untrusted user input is concatenated directly into database queries. This could allow an attacker to view, modify, or delete database data.",
            "impact": "Critial data loss or unauthorized access."
        },
        "Cross-Site Scripting (Reflected)": {
            "text": "The application reflects user input without adequate encoding, potentially allowing the execution of malicious scripts in the victim's browser. This is often used to steal session cookies or perform actions on behalf of the user.",
            "impact": "Session hijacking or unauthorized actions."
        },
        "Missing Security Headers": {
            "text": "The application response does not include one or more recommended HTTP security headers (such as Content-Security-Policy, X-Frame-Options, or X-Content-Type-Options). While this does not constitute an exploitable vulnerability on its own, missing headers can reduce browser-level protections against clickjacking, XSS, and MIME-type attacks.",
            "impact": "Reduced defense-in-depth against client-side attacks."
        },
        "Technology Fingerprinting": {
            "text": "The application reveals specific version information about the underlying server or framework (e.g., via 'Server' or 'X-Powered-By' headers). This information facilitates targeted attacks by helping adversaries identify known vulnerabilities associated with specific versions.",
            "impact": "Information disclosure aiding reconnaissance."
        },
        "Redirected Path": {
            "text": "The requested resource redirected the client to another location (e.g., a login page or home page). This confirms the existence of the endpoint logic but indicates that direct access is managed or restricted.",
            "impact": "Informational; maps application flow."
        },
        "Access Controlled Resource": {
            "text": "The application explicitly denied access to the requested resource (HTTP 403 Forbidden or 401 Unauthorized). This is a positive security finding indicating that access controls are likely active and enforcing restrictions.",
            "impact": "Informational; confirms access control presence."
        },
        "Sensitive File Exposure": {
            "text": "The application returned a valid response containing sensitive keywords or patterns (e.g., passwords, API keys, or configuration data). This indicates a high probability of sensitive data leakage.",
            "impact": "Critical information disclosure."
        },
        "Sensitive Path Probe": {
            "text": "The application returned a valid 200 OK response for a potentially sensitive endpoint, but the content did not contain immediate proof of sensitive data. This suggests the resource exists and is accessible, warranting manual review.",
            "impact": "Potential information disclosure."
        },
        "Public Information": {
            "text": "The application exposes standard informational files (e.g., robots.txt, security.txt). These files are intended for public consumption and are not vulnerabilities, but they provide context about the application's configuration.",
            "impact": "Informational context."
        }
    }

    @classmethod
    def get_explanation(cls, vuln_type: str) -> str:
        entry = cls.EXPLANATIONS.get(vuln_type)
        if entry:
            return f"{entry['text']} Impact: {entry['impact']}"
        return "Automated finding requiring manual verification."


# ==============================================================================
# DATA MODELS
# ==============================================================================

@dataclass
class HttpSnapshot:
    url: str
    status_code: int
    elapsed_ms: int
    content_len: int
    body_hash: str
    headers: Dict[str, str] = field(default_factory=dict)


@dataclass
class Finding:
    timestamp: str
    vuln_type: str
    severity: str
    confidence: str
    target_url: str
    parameter: str
    http_method: str
    payload: str
    evidence: Dict[str, Any]
    mitigation: str
    cwe_id: str = ""
    owasp_category: str = ""
    finding_status: str = "ENUMERATED"
    http_status: int = 0
    content_type: str = ""
    final_url: str = ""
    body_fingerprint: str = ""


@dataclass
class DorkResult:
    """Result from a Google dork query."""
    dork_query: str
    urls_found: List[str]
    timestamp: str
    method: str  # "library" or "manual"


# ==============================================================================
# GOOGLE DORKING MODULE
# ==============================================================================

class GoogleDorker:
    """
    Google Dorking for OSINT reconnaissance.
    Implements both library-based and manual scraping approaches.
    """
    
    # User-Agent rotation to avoid detection
    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0',
        'Mozilla/5.0 (X11; Linux x86_64; rv:121.0) Gecko/20100101 Firefox/121.0',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
    ]
    
    # Security-focused Google dorks
    SECURITY_DORKS = {
        'admin_panels': [
            'site:{domain} inurl:admin',
            'site:{domain} inurl:administrator',
            'site:{domain} intitle:"admin panel"',
            'site:{domain} intitle:"dashboard" inurl:admin',
        ],
        'login_pages': [
            'site:{domain} inurl:login',
            'site:{domain} inurl:signin',
            'site:{domain} intitle:"login"',
        ],
        'config_files': [
            'site:{domain} filetype:env',
            'site:{domain} filetype:config',
            'site:{domain} filetype:ini',
            'site:{domain} inurl:configuration',
        ],
        'database_info': [
            'site:{domain} filetype:sql',
            'site:{domain} "sql syntax"',
            'site:{domain} "mysql" ext:sql',
        ],
        'directory_listings': [
            'site:{domain} intitle:"index of"',
            'site:{domain} intitle:"directory listing"',
            'site:{domain} intitle:"index of /" "parent directory"',
        ],
        'sensitive_docs': [
            'site:{domain} filetype:pdf',
            'site:{domain} filetype:doc | filetype:docx',
            'site:{domain} filetype:xls | filetype:xlsx',
        ],
        'error_pages': [
            'site:{domain} "warning:"',
            'site:{domain} "error:"',
            'site:{domain} "fatal error"',
            'site:{domain} "stack trace"',
        ],
        'subdomains': [
            'site:*.{domain}',
            'site:*.*.{domain}',
        ],
    }
    
    
    def __init__(
        self, 
        domain: str, 
        max_results_per_dork: int = 10, 
        delay_range: tuple = (2, 5),
        api_key: str = None,
        cse_id: str = None,
        max_retries: int = 3
    ):
        self.domain = domain
        self.max_results = max_results_per_dork
        self.delay_min, self.delay_max = delay_range
        self.max_retries = max_retries
        self.results: List[DorkResult] = []
        
        # Google Custom Search API credentials
        self.api_key = api_key
        self.cse_id = cse_id
        self.has_api = bool(api_key and cse_id)
        
        # Try to import Google API client
        self.google_api = None
        if self.has_api:
            try:
                from googleapiclient.discovery import build
                self.google_api = build
            except ImportError:
                print(f"  {Colors.YELLOW}[!] google-api-python-client not installed. API mode disabled.{Colors.END}")
                print(f"  {Colors.CYAN}Install with: pip install google-api-python-client{Colors.END}")
                self.has_api = False
        
        # Try to import googlesearch library
        self.has_library = False
        try:
            from googlesearch import search as google_search
            self.google_search = google_search
            self.has_library = True
        except ImportError:
            pass
    
    def _get_random_user_agent(self) -> str:
        """Get a random user agent."""
        return random.choice(self.USER_AGENTS)
    
    def _random_delay(self) -> None:
        """Random delay to avoid rate limiting."""
        time.sleep(random.uniform(self.delay_min, self.delay_max))
    
    def manual_search(self, query: str, num_results: int = 10) -> List[str]:
        """
        Manual Google scraping with exponential backoff (educational).
        WARNING: Against Google ToS, may get blocked.
        """
        urls = []
        
        for attempt in range(self.max_retries):
            try:
                headers = {
                    'User-Agent': self._get_random_user_agent(),
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.5',
                    'Accept-Encoding': 'gzip, deflate',
                    'DNT': '1',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                }
                
                params = {
                    'q': query,
                    'num': num_results,
                    'hl': 'en',
                }
                
                # Exponential backoff: wait longer on each retry
                if attempt > 0:
                    backoff_time = (2 ** attempt) + random.uniform(1, 3)
                    print(f"  {Colors.YELLOW}[*] Retry {attempt}/{self.max_retries} after {backoff_time:.1f}s backoff{Colors.END}")
                    time.sleep(backoff_time)
                else:
                    self._random_delay()
                
                response = requests.get(
                    'https://www.google.com/search',
                    params=params,
                    headers=headers,
                    timeout=15
                )
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    for link in soup.find_all('a', href=True):
                        href = link['href']
                        if '/url?q=' in href:
                            url = href.split('/url?q=')[1].split('&')[0]
                            from urllib.parse import unquote
                            url = unquote(url)
                            if not any(x in url for x in ['google.com', 'webcache', 'translate.google']):
                                if url.startswith('http'):
                                    urls.append(url)
                                    if len(urls) >= num_results:
                                        break
                    return urls
                
                elif response.status_code == 429:
                    print(f"  {Colors.YELLOW}[!] Rate limited (429) - attempt {attempt + 1}/{self.max_retries}{Colors.END}")
                    if attempt < self.max_retries - 1:
                        continue
                    else:
                        print(f"  {Colors.RED}[!] Max retries reached{Colors.END}")
                else:
                    print(f"  {Colors.YELLOW}[!] Google returned status {response.status_code}{Colors.END}")
                    break
            
            except Exception as e:
                print(f"  {Colors.RED}[!] Manual scraping error: {str(e)}{Colors.END}")
                if attempt < self.max_retries - 1:
                    continue
        
        return urls
    
    def library_search(self, query: str, num_results: int = 10) -> List[str]:
        """
        Use googlesearch-python library.
        More reliable but still subject to rate limits.
        """
        urls = []
        
        try:
            # Add delay before searching
            self._random_delay()
            
            # Use the library
            for url in self.google_search(query, num_results=num_results, lang='en'):
                urls.append(url)
                if len(urls) >= num_results:
                    break
        
        except Exception as e:
            print(f"  {Colors.YELLOW}[!] Library search error: {str(e)}{Colors.END}")
        
        return urls
    
    def api_search(self, query: str, num_results: int = 10) -> List[str]:
        """
        Use Google Custom Search API (professional, no rate limits).
        Requires API key and CSE ID. 100 free queries/day, then $5/1000.
        """
        urls = []
        
        if not self.has_api:
            return urls
        
        try:
            # Build the Custom Search service
            service = self.google_api("customsearch", "v1", developerKey=self.api_key)
            
            # Execute search (max 10 results per request due to API limit)
            batch_size = min(num_results, 10)
            res = service.cse().list(
                q=query,
                cx=self.cse_id,
                num=batch_size
            ).execute()
            
            # Extract URLs
            if 'items' in res:
                for item in res['items']:
                    urls.append(item['link'])
                    if len(urls) >= num_results:
                        break
            
            print(f"  {Colors.GREEN}[+] API search successful: {len(urls)} results{Colors.END}")
        
        except Exception as e:
            print(f"  {Colors.RED}[!] API search error: {str(e)}{Colors.END}")
            # Common errors:
            # - Invalid API key
            # - Daily quota exceeded (100 free/day)
            # - Invalid CSE ID
        
        return urls
    
    def smart_search(self, query: str, num_results: int = 10) -> tuple[List[str], str]:
        """
        Professional cascading fallback: API → Library → Manual.
        Returns: (urls, method_used)
        """
        urls = []
        method = "none"
        
        # Try API first if available (best: no rate limits)
        if self.has_api:
            urls = self.api_search(query, num_results)
            if urls:
                method = "api"
                return urls, method
        
        # Try library if available (good: better than manual)
        if self.has_library:
            urls = self.library_search(query, num_results)
            if urls:
                method = "library"
                return urls, method
        
        # Fallback to manual scraping (last resort: educational)
        urls = self.manual_search(query, num_results)
        if urls:
            method = "manual"
        
        return urls, method
    
    def run_dorks(self, categories: List[str] = None, custom_dorks: List[str] = None) -> List[DorkResult]:
        """
        Run Google dorks for specified categories.
        
        Args:
            categories: List of dork categories to run (e.g., ['admin_panels', 'login_pages'])
            custom_dorks: List of custom dork queries to run
        
        Returns:
            List of DorkResult objects
        """
        results = []
        
        # Default to all categories if none specified
        if categories is None:
            categories = list(self.SECURITY_DORKS.keys())
        
        # Run predefined dorks
        for category in categories:
            if category not in self.SECURITY_DORKS:
                continue
            
            dork_templates = self.SECURITY_DORKS[category]
            
            for dork_template in dork_templates:
                # Replace {domain} placeholder
                dork = dork_template.format(domain=self.domain)
                
                # Run search
                urls, method = self.smart_search(dork, self.max_results)
                
                if urls:
                    result = DorkResult(
                        dork_query=dork,
                        urls_found=urls,
                        timestamp=iso_now(),
                        method=method
                    )
                    results.append(result)
        
        # Run custom dorks if provided
        if custom_dorks:
            for dork in custom_dorks:
                urls, method = self.smart_search(dork, self.max_results)
                
                if urls:
                    result = DorkResult(
                        dork_query=dork,
                        urls_found=urls,
                        timestamp=iso_now(),
                        method=method
                    )
                    results.append(result)
        
        self.results = results
        return results
    
    def get_all_urls(self) -> Set[str]:
        """Get all unique URLs discovered from all dorks."""
        all_urls = set()
        for result in self.results:
            all_urls.update(result.urls_found)
        return all_urls
    
    def print_results(self) -> None:
        """Print dork results to console."""
        if not self.results:
            print(f"  {Colors.YELLOW}No results found{Colors.END}")
            return
        
        total_urls = len(self.get_all_urls())
        print(f"\n  {Colors.GREEN}Total unique URLs discovered: {total_urls}{Colors.END}")
        
        for result in self.results:
            method_color = Colors.GREEN if result.method == "library" else Colors.CYAN
            print(f"\n  {Colors.BOLD}Dork:{Colors.END} {result.dork_query}")
            print(f"  {method_color}Method: {result.method}{Colors.END}")
            print(f"  Found {len(result.urls_found)} URLs:")
            for url in result.urls_found[:5]:  # Show first 5
                print(f"    • {url}")
            if len(result.urls_found) > 5:
                print(f"    ... and {len(result.urls_found) - 5} more")



@dataclass
class ScanStatistics:
    start_time: str
    end_time: str = ""
    urls_crawled: int = 0
    requests_made: int = 0
    findings_count: int = 0
    sqli_findings: int = 0
    xss_findings: int = 0
    header_findings: int = 0
    exposure_findings: int = 0
    cmdi_findings: int = 0
    lfi_findings: int = 0
    cors_findings: int = 0
    cookie_findings: int = 0
    tech_findings: int = 0
    dorking_urls_found: int = 0


# ==============================================================================
# LOGGING & PERSISTENCE
# ==============================================================================

def write_finding_jsonl(path: str, finding: dict, lock: Lock = None) -> None:
    """Thread-safe JSONL logging."""
    os.makedirs(os.path.dirname(path) if os.path.dirname(path) else ".", exist_ok=True)
    
    if lock:
        with lock:
            with open(path, "a", encoding="utf-8") as f:
                f.write(json.dumps(finding, ensure_ascii=False) + "\n")
    else:
        with open(path, "a", encoding="utf-8") as f:
            f.write(json.dumps(finding, ensure_ascii=False) + "\n")


def iso_now() -> str:
    return dt.datetime.now(dt.timezone.utc).isoformat()


def stable_hash(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8", errors="ignore")).hexdigest()[:16]


# ==============================================================================
# MITIGATION DATABASE
# ==============================================================================

class MitigationDatabase:
    """Provides remediation guidance for detected vulnerabilities."""
    
    MITIGATIONS = {
        "SQLi": {
            "title": "SQL Injection Remediation",
            "description": "SQL Injection occurs when untrusted data is sent to an interpreter as part of a command or query.",
            "fix_code": '''
# ❌ VULNERABLE CODE (Never do this):
user_input = request.form['username']
cursor.execute(f"SELECT * FROM users WHERE user = '{user_input}'")

# ✅ SECURE FIX (Always use parameterized queries):
user_input = request.form['username']
cursor.execute("SELECT * FROM users WHERE user = %s", (user_input,))
''',
            "additional_controls": [
                "Use stored procedures with parameterized inputs",
                "Implement least-privilege database accounts",
                "Suppress verbose database errors in production",
                "Deploy a Web Application Firewall (WAF)",
            ],
            "cwe": "CWE-89",
            "owasp": "A03:2021 - Injection",
        },
        "XSS": {
            "title": "Cross-Site Scripting (XSS) Remediation",
            "description": "XSS flaws occur when an application includes untrusted data without proper validation or escaping.",
            "fix_code": '''
# ❌ VULNERABLE CODE:
return f"<html><body>Hello, {user_input}</body></html>"

# ✅ SECURE FIX (Use contextual output encoding):
from markupsafe import escape
return f"<html><body>Hello, {escape(user_input)}</body></html>"
''',
            "additional_controls": [
                "Implement Content Security Policy (CSP) headers",
                "Use HTTPOnly and Secure flags on cookies",
                "Validate and sanitize all user inputs",
            ],
            "cwe": "CWE-79",
            "owasp": "A03:2021 - Injection",
        },
        "CMDI": {
            "title": "Command Injection Remediation",
            "description": "Command injection allows execution of arbitrary OS commands on the server.",
            "fix_code": '''
# ❌ VULNERABLE CODE:
import os
filename = request.form['file']
os.system(f"cat {filename}")

# ✅ SECURE FIX (Avoid shell commands, use safe alternatives):
import subprocess
filename = request.form['file']
# Validate filename first
if not re.match(r'^[a-zA-Z0-9_.-]+$', filename):
    raise ValueError("Invalid filename")
subprocess.run(['cat', filename], check=True)
''',
            "additional_controls": [
                "Never pass user input directly to shell commands",
                "Use allowlists for permitted values",
                "Avoid shell=True in subprocess calls",
                "Implement strict input validation",
            ],
            "cwe": "CWE-78",
            "owasp": "A03:2021 - Injection",
        },
        "LFI": {
            "title": "Path Traversal / LFI Remediation",
            "description": "Path traversal allows attackers to access files outside the intended directory.",
            "fix_code": '''
# ❌ VULNERABLE CODE:
filepath = request.form['file']
with open(f"/var/www/docs/{filepath}") as f:
    return f.read()

# ✅ SECURE FIX (Validate and normalize paths):
import os
from pathlib import Path
base_dir = Path("/var/www/docs")
requested = request.form['file']
filepath = (base_dir / requested).resolve()
if not filepath.is_relative_to(base_dir):
    raise ValueError("Access denied")
return filepath.read_text()
''',
            "additional_controls": [
                "Use allowlists for permitted files",
                "Avoid user input in file paths entirely",
                "Use chroot jails or containers",
                "Validate and sanitize all file paths",
            ],
            "cwe": "CWE-22",
            "owasp": "A01:2021 - Broken Access Control",
        },
        "CORS": {
            "title": "CORS Misconfiguration Remediation",
            "description": "Overly permissive CORS policies can expose sensitive data to malicious origins.",
            "fix_code": '''
# ❌ VULNERABLE CODE:
Access-Control-Allow-Origin: *
Access-Control-Allow-Credentials: true

# ✅ SECURE FIX (Use strict origin allowlist):
allowed_origins = ['https://trusted-site.com', 'https://app.example.com']
origin = request.headers.get('Origin')
if origin in allowed_origins:
    response.headers['Access-Control-Allow-Origin'] = origin
    response.headers['Access-Control-Allow-Credentials'] = 'true'
''',
            "additional_controls": [
                "Never use wildcard (*) with credentials",
                "Validate Origin header against allowlist",
                "Avoid reflecting Origin without validation",
                "Use specific origins, not patterns",
            ],
            "cwe": "CWE-346",
            "owasp": "A05:2021 - Security Misconfiguration",
        },
        "COOKIE_SECURITY": {
            "title": "Cookie Security Configuration",
            "description": "Missing security flags on cookies can lead to session hijacking and XSS attacks.",
            "fix_code": '''
# ❌ VULNERABLE CODE:
Set-Cookie: sessionid=abc123; Path=/

# ✅ SECURE FIX (Use all security flags):
Set-Cookie: sessionid=abc123; Path=/; HttpOnly; Secure; SameSite=Strict
''',
            "additional_controls": [
                "Always set HttpOnly for session cookies",
                "Always set Secure flag (HTTPS only)",
                "Use SameSite=Strict or Lax",
                "Set appropriate expiration times",
            ],
            "cwe": "CWE-614",
            "owasp": "A05:2021 - Security Misconfiguration",
        },
        "SECURITY_HEADERS": {
            "title": "Security Headers Configuration",
            "description": "Missing or misconfigured security headers can expose the application to various attacks.",
            "fix_code": '''
# Nginx configuration:
add_header X-Frame-Options "DENY" always;
add_header X-Content-Type-Options "nosniff" always;
add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;
add_header Content-Security-Policy "default-src 'self';" always;
''',
            "additional_controls": [
                "Regularly audit security headers",
                "Implement a strict Content Security Policy",
                "Enable HSTS preloading for your domain",
            ],
            "cwe": "CWE-693",
            "owasp": "A05:2021 - Security Misconfiguration",
        },
        "SENSITIVE_EXPOSURE": {
            "title": "Sensitive File Exposure Remediation",
            "description": "Sensitive files accessible via web can leak critical information.",
            "fix_code": '''
# Nginx - Block sensitive paths:
location ~ /\\. { deny all; }
location ~ /(config|backup|admin) { deny all; return 404; }
''',
            "additional_controls": [
                "Remove development/debug files before deployment",
                "Use environment variables for secrets",
                "Implement proper access controls",
            ],
            "cwe": "CWE-538",
            "owasp": "A01:2021 - Broken Access Control",
        },
    }
    
    @classmethod
    def get_short_mitigation(cls, vuln_type: str) -> str:
        if vuln_type not in cls.MITIGATIONS:
            return "Apply secure coding practices."
        m = cls.MITIGATIONS[vuln_type]
        return f"{m['title']}: {m['additional_controls'][0]}. See {m['cwe']}."
    
    @classmethod
    def get_cwe(cls, vuln_type: str) -> str:
        return cls.MITIGATIONS.get(vuln_type, {}).get("cwe", "")
    
    @classmethod
    def get_owasp(cls, vuln_type: str) -> str:
        return cls.MITIGATIONS.get(vuln_type, {}).get("owasp", "")


# ==============================================================================
# CORE PENTEST AGENT (Multi-threaded)
# ==============================================================================

class PentestAgent:
    """
    Autonomous vulnerability detection agent with multi-threading support.
    """
    
    # Detection Patterns
    DB_ERROR_PATTERNS = [
        re.compile(r"SQLSTATE\[\w+\]", re.IGNORECASE),
        re.compile(r"you have an error in your sql syntax", re.IGNORECASE),
        re.compile(r"mysql_fetch_", re.IGNORECASE),
        re.compile(r"unclosed quotation mark", re.IGNORECASE),
        re.compile(r"odbc.*driver", re.IGNORECASE),
        re.compile(r"pg_query\(", re.IGNORECASE),
        re.compile(r"syntax error at or near", re.IGNORECASE),
        re.compile(r"sqlite.*error", re.IGNORECASE),
        re.compile(r"warning:\s*mysql", re.IGNORECASE),
        re.compile(r"ORA-\d{5}", re.IGNORECASE),
        re.compile(r"Microsoft SQL Native Client", re.IGNORECASE),
        re.compile(r"PostgreSQL.*ERROR", re.IGNORECASE),
        re.compile(r"Driver.*SQL.*Server", re.IGNORECASE),
        re.compile(r"quoted string not properly terminated", re.IGNORECASE),
    ]
    
    SQLI_PAYLOADS_ERROR = ["'", '"', "')", '")', "' OR '1'='1", "' AND '1'='2", "1' ORDER BY 1--"]
    SQLI_PAYLOADS_TIME = ["' OR SLEEP(3)--", "'; WAITFOR DELAY '0:0:3'--", "' OR pg_sleep(3)--"]
    
    XSS_PAYLOADS = [
        "<script>alert('XSS')</script>",
        "<img src=x onerror=alert('XSS')>",
        "'\"><script>alert('XSS')</script>",
        "<svg/onload=alert('XSS')>",
    ]
    XSS_DETECTION_MARKER = "alert('XSS')"
    
    SENSITIVE_PATHS = [
        # Version Control
        "/.git/config", "/.git/HEAD", "/.git/index", "/.gitignore",
        "/.svn/entries", "/.svn/wc.db", "/.hg/hgrc", "/.bzr/README",
        
        # Environment & Config
        "/.env", "/.env.local", "/.env.production", "/.env.backup",
        "/config.php", "/config.inc.php", "/configuration.php",
        "/settings.php", "/settings.py", "/config.json", "/config.yaml",
        "/config.yml", "/app.config", "/web.config", "/appsettings.json",
        
        # CMS Config
        "/wp-config.php", "/wp-config.php.bak", "/wp-config.php.old",
        "/wp-config.php.save", "/wp-config.php.swp", "/wp-config.php~",
        "/sites/default/settings.php",  # Drupal
        "/configuration.php",  # Joomla
        
        # Debug & Info
        "/phpinfo.php", "/info.php", "/test.php", "/debug.php",
        "/server-status", "/server-info", "/.htaccess", "/.htpasswd",
        "/elmah.axd", "/trace.axd",  # ASP.NET
        
        # Admin & Backend
        "/admin/", "/administrator/", "/admin.php", "/login.php",
        "/panel/", "/cpanel/", "/phpmyadmin/", "/pma/", "/adminer.php",
        "/manager/", "/console/", "/dashboard/",
        
        # Backup Files
        "/backup/", "/backups/", "/backup.sql", "/backup.zip",
        "/backup.tar.gz", "/db.sql", "/database.sql", "/dump.sql",
        "/site.zip", "/www.zip", "/html.zip", "/.backup",
        
        # Log Files
        "/logs/", "/log/", "/error.log", "/access.log", "/debug.log",
        "/app.log", "/application.log", "/error_log", "/errors.log",
        
        # Cloud & Credentials
        "/.aws/credentials", "/.aws/config",
        "/.docker/config.json", "/docker-compose.yml",
        "/.kube/config", "/.ssh/id_rsa", "/.ssh/known_hosts",
        "/id_rsa", "/id_dsa", "/.npmrc", "/.netrc",
        
        # API & Documentation
        "/api/", "/api/v1/", "/api/v2/", "/graphql",
        "/swagger.json", "/swagger.yaml", "/openapi.json",
        "/api-docs/", "/docs/", "/documentation/",
        
        # Development Files
        "/.DS_Store", "/Thumbs.db", "/.idea/", "/.vscode/",
        "/composer.json", "/composer.lock", "/package.json",
        "/package-lock.json", "/yarn.lock", "/Gemfile", "/requirements.txt",
        
        # Common Sensitive
        "/robots.txt", "/sitemap.xml", "/crossdomain.xml",
        "/clientaccesspolicy.xml", "/security.txt", "/.well-known/security.txt",
        "/humans.txt", "/readme.html", "/readme.txt", "/README.md",
        "/CHANGELOG.md", "/LICENSE", "/version.txt",
        
        # Database Files
        "/db.sqlite", "/database.sqlite", "/data.db", "/app.db",
        "/.sqlite_history", "/phpliteadmin.php",
        
        # Server Files  
        "/nginx.conf", "/apache2.conf", "/httpd.conf",
        "/.user.ini", "/php.ini", "/.htaccess.bak",
    ]
    
    # Command Injection Payloads
    CMDI_PAYLOADS = [
        "; sleep 3",
        "| sleep 3",
        "& timeout /t 3",
        "`sleep 3`",
        "$(sleep 3)",
        "; ping -c 3 127.0.0.1",
    ]
    CMDI_OUTPUT_PAYLOADS = [
        "; whoami",
        "| whoami",
        "; cat /etc/passwd",
        "& ver",
    ]
    
    # Path Traversal / LFI Payloads
    LFI_PAYLOADS = [
        # Basic Linux traversal (various depths)
        "../etc/passwd",
        "../../etc/passwd",
        "../../../etc/passwd",
        "../../../../etc/passwd",
        "../../../../../etc/passwd",
        "../../../../../../etc/passwd",
        "../../../../../../../etc/passwd",
        
        # Basic Windows traversal
        "..\\windows\\win.ini",
        "..\\..\\windows\\win.ini",
        "..\\..\\..\\windows\\win.ini",
        "..\\..\\..\\..\\windows\\win.ini",
        "..\\..\\..\\windows\\system32\\drivers\\etc\\hosts",
        "..\\..\\..\\..\\boot.ini",
        
        # Mixed slashes (bypass attempts)
        "....//....//....//etc/passwd",
        "..../..../..../etc/passwd",
        "..\\../..\\../etc/passwd",
        "..\\..\\..\\/etc/passwd",
        
        # URL encoding (single)
        "%2e%2e%2fetc%2fpasswd",
        "%2e%2e%2f%2e%2e%2fetc%2fpasswd",
        "%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd",
        
        # URL encoding (double)
        "%252e%252e%252fetc%252fpasswd",
        "..%252f..%252f..%252fetc%252fpasswd",
        "%252e%252e%252f%252e%252e%252fetc%252fpasswd",
        
        # Unicode/UTF-8 encoding
        "..%c0%af..%c0%afetc%c0%afpasswd",
        "..%c1%9c..%c1%9cetc%c1%9cpasswd",
        "%c0%ae%c0%ae/%c0%ae%c0%ae/etc/passwd",
        
        # Null byte injection (may work on older PHP)
        "../../../etc/passwd%00",
        "../../../etc/passwd%00.jpg",
        "../../../etc/passwd%00.php",
        "....//....//....//etc/passwd%00",
        
        # Absolute paths (Linux)
        "/etc/passwd",
        "/etc/shadow",
        "/etc/hosts",
        "/etc/hostname",
        "/etc/issue",
        "/etc/group",
        "/proc/self/environ",
        "/proc/version",
        "/proc/self/cmdline",
        "/var/log/apache2/access.log",
        "/var/log/apache2/error.log",
        "/var/log/nginx/access.log",
        "/var/log/auth.log",
        "/var/log/syslog",
        
        # Absolute paths (Windows)
        "C:\\Windows\\win.ini",
        "C:\\Windows\\System32\\drivers\\etc\\hosts",
        "C:\\boot.ini",
        "C:\\Windows\\system.ini",
        
        # PHP Wrappers (for PHP apps)
        "php://filter/convert.base64-encode/resource=index.php",
        "php://filter/convert.base64-encode/resource=config.php",
        "php://filter/convert.base64-encode/resource=../config.php",
        "php://input",
        "data://text/plain;base64,PD9waHAgc3lzdGVtKCRfR0VUWydjbWQnXSk7Pz4=",
        "expect://id",
        
        # Wrapper with encoding bypass
        "php://filter/read=convert.base64-encode/resource=../../../etc/passwd",
        "pHp://FilTer/convert.base64-encode/resource=index.php",
        
        # File:// wrapper
        "file:///etc/passwd",
        "file://C:/Windows/win.ini",
    ]
    
    LFI_SIGNATURES = [
        # Linux /etc/passwd patterns
        re.compile(r"root:[x*]:0:0:", re.IGNORECASE),
        re.compile(r"daemon:[x*]:\d+:\d+:", re.IGNORECASE),
        re.compile(r"nobody:[x*]:\d+:\d+:", re.IGNORECASE),
        re.compile(r"www-data:", re.IGNORECASE),
        re.compile(r"bin:[x*]:\d+:\d+:", re.IGNORECASE),
        
        # Windows win.ini patterns
        re.compile(r"\[extensions\]", re.IGNORECASE),
        re.compile(r"\[fonts\]", re.IGNORECASE),
        re.compile(r"\[mci extensions\]", re.IGNORECASE),
        
        # Windows hosts file
        re.compile(r"127\.0\.0\.1\s+localhost", re.IGNORECASE),
        
        # Windows boot.ini
        re.compile(r"\[boot loader\]", re.IGNORECASE),
        
        # Linux /etc/shadow (if accessible)
        re.compile(r"root:\$[156]\$", re.IGNORECASE),
        
        # PHP file content (base64 decode indicators)
        re.compile(r"<\?php", re.IGNORECASE),
        re.compile(r"<\?=", re.IGNORECASE),
        
        # Linux version info
        re.compile(r"Linux version \d+\.\d+", re.IGNORECASE),
        
        # Process environ
        re.compile(r"PATH=", re.IGNORECASE),
        re.compile(r"HOME=/", re.IGNORECASE),
    ]
    
    # Technology Fingerprinting Signatures
    TECH_SIGNATURES = {
        "servers": {
            re.compile(r"apache/(\d+\.\d+)", re.IGNORECASE): "Apache",
            re.compile(r"nginx/(\d+\.\d+)", re.IGNORECASE): "Nginx",
            re.compile(r"Microsoft-IIS/(\d+\.\d+)", re.IGNORECASE): "IIS",
        },
        "frameworks": {
            re.compile(r"X-Powered-By: PHP/(\d+\.\d+)", re.IGNORECASE): "PHP",
            re.compile(r"X-AspNet-Version", re.IGNORECASE): "ASP.NET",
            re.compile(r"X-Powered-By: Express", re.IGNORECASE): "Express.js",
            re.compile(r"django", re.IGNORECASE): "Django",
        },
        "cms": {
            re.compile(r"/wp-content/", re.IGNORECASE): "WordPress",
            re.compile(r"/sites/default/", re.IGNORECASE): "Drupal",
            re.compile(r"/components/com_", re.IGNORECASE): "Joomla",
        },
    }
    
    SECURITY_HEADERS = {
        "Strict-Transport-Security": ("Missing HSTS header", Severity.MEDIUM),
        "X-Frame-Options": ("Missing X-Frame-Options (clickjacking risk)", Severity.MEDIUM),
        "X-Content-Type-Options": ("Missing X-Content-Type-Options", Severity.LOW),
        "Content-Security-Policy": ("Missing Content-Security-Policy", Severity.MEDIUM),
        "X-XSS-Protection": ("Missing X-XSS-Protection", Severity.LOW),
        "Referrer-Policy": ("Missing Referrer-Policy", Severity.INFO),
        "Permissions-Policy": ("Missing Permissions-Policy", Severity.INFO),
    }
    
    def __init__(
        self,
        base_url: str,
        *,
        verify_tls: bool = False,
        timeout_s: int = DEFAULT_TIMEOUT,
        delay_s: float = DEFAULT_DELAY,
        max_requests: int = DEFAULT_MAX_REQUESTS,
        max_depth: int = 3,
        out_file: str = DEFAULT_LOG_FILE,
        user_agent: str = "MastersProject-SecurityScanner/4.0",
        enable_sqli: bool = True,
        enable_xss: bool = True,
        enable_headers: bool = True,
        enable_exposure: bool = True,
        enable_cmdi: bool = True,
        enable_lfi: bool = True,
        enable_cors: bool = True,
        enable_cookies: bool = True,
        enable_tech: bool = True,
        enable_dorking: bool = False,  # Off by default (can be slow)
        time_based_sqli: bool = False,
        verbose: bool = False,
        threads: int = DEFAULT_THREADS,
    ):
        # Normalize URL
        if not base_url.startswith(('http://', 'https://')):
            base_url = 'https://' + base_url
        self.base_url = base_url.rstrip("/")
        self.parsed_base = urlparse(self.base_url)
        
        self.verify_tls = verify_tls
        self.timeout_s = timeout_s
        self.delay_s = delay_s
        self.max_requests = max_requests
        self.max_depth = max_depth
        self.out_file = out_file
        self.time_based_sqli = time_based_sqli
        self.verbose = verbose
        self.threads = threads
        
        # Module toggles
        self.enable_sqli = enable_sqli
        self.enable_xss = enable_xss
        self.enable_headers = enable_headers
        self.enable_exposure = enable_exposure
        self.enable_cmdi = enable_cmdi
        self.enable_lfi = enable_lfi
        self.enable_cors = enable_cors
        self.enable_cookies = enable_cookies
        self.enable_tech = enable_tech
        self.enable_dorking = enable_dorking
        
        # Session setup
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": user_agent,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
        })
        
        # Thread-safe tracking
        self._request_count = 0
        self._request_lock = Lock()
        self._findings_lock = Lock()
        self._log_lock = Lock()
        self._findings: List[Finding] = []
        self._not_found_paths: List[str] = []
        self._stats = ScanStatistics(start_time=iso_now())
        
        self._print_config()
    
    def _print_config(self):
        """Display current configuration."""
        print(f"\n{Colors.BOLD}{Colors.CYAN}═══════════════════════════════════════════════════════════════════════════════")
        print(f"                           SCAN CONFIGURATION")
        print(f"═══════════════════════════════════════════════════════════════════════════════{Colors.END}")
        print(f"  {Colors.GREEN}Target:{Colors.END}          {self.base_url}")
        print(f"  {Colors.GREEN}Max Requests:{Colors.END}   {self.max_requests}")
        print(f"  {Colors.GREEN}Threads:{Colors.END}        {self.threads}")
        print(f"  {Colors.GREEN}Crawl Depth:{Colors.END}    {self.max_depth}")
        print(f"  {Colors.GREEN}Timeout:{Colors.END}        {self.timeout_s}s")
        print(f"  {Colors.GREEN}Delay:{Colors.END}          {self.delay_s}s")
        print(f"  {Colors.GREEN}Output File:{Colors.END}    {self.out_file}")
        print(f"\n  {Colors.BOLD}Enabled Modules:{Colors.END}")
        print(f"  {'✅' if self.enable_sqli else '❌'} SQL Injection")
        print(f"  {'✅' if self.enable_xss else '❌'} XSS")
        print(f"  {'✅' if self.enable_cmdi else '❌'} Command Injection")
        print(f"  {'✅' if self.enable_lfi else '❌'} Path Traversal/LFI")
        print(f"  {'✅' if self.enable_cors else '❌'} CORS Misconfiguration")
        print(f"  {'✅' if self.enable_cookies else '❌'} Cookie Security")
        print(f"  {'✅' if self.enable_tech else '❌'} Technology Fingerprinting")
        print(f"  {'✅' if self.enable_headers else '❌'} Security Headers")
        print(f"  {'✅' if self.enable_exposure else '❌'} File Exposure")
        print(f"  {'✅' if self.enable_dorking else '❌'} Google Dorking (OSINT)")
        if self.time_based_sqli:
            print(f"  ✅ Time-Based Blind SQLi")
        print(f"{Colors.CYAN}═══════════════════════════════════════════════════════════════════════════════{Colors.END}\n")
    
    # -------------------------------------------------------------------------
    # HTTP Helpers (Thread-safe)
    # -------------------------------------------------------------------------
    
    def _increment_requests(self) -> bool:
        """Thread-safe request counter. Returns False if limit reached."""
        with self._request_lock:
            if self._request_count >= self.max_requests:
                return False
            self._request_count += 1
            self._stats.requests_made = self._request_count
            return True
    
    def _throttle(self) -> None:
        if self.delay_s > 0:
            time.sleep(self.delay_s)
    
    def _request(self, method: str, url: str, **kwargs) -> Optional[requests.Response]:
        if not self._increment_requests():
            return None
        
        self._throttle()
        kwargs.setdefault("verify", self.verify_tls)
        kwargs.setdefault("timeout", self.timeout_s)
        kwargs.setdefault("allow_redirects", True)
        
        try:
            return self.session.request(method, url, **kwargs)
        except requests.RequestException as e:
            if self.verbose:
                logging.debug("Request failed: %s - %s", url, str(e)[:100])
            return None
    
    def _get(self, url: str, **kwargs) -> Optional[requests.Response]:
        return self._request("GET", url, **kwargs)
    
    def _snapshot(self, url: str) -> Optional[HttpSnapshot]:
        start = time.time()
        resp = self._get(url)
        if resp is None:
            return None
        elapsed_ms = int((time.time() - start) * 1000)
        text = resp.text or ""
        return HttpSnapshot(
            url=resp.url,
            status_code=resp.status_code,
            elapsed_ms=elapsed_ms,
            content_len=len(text),
            body_hash=stable_hash(text),
            headers={k.lower(): v for k, v in resp.headers.items()},
        )
    
    # -------------------------------------------------------------------------
    # URL Helpers
    # -------------------------------------------------------------------------
    
    def _same_host(self, url: str) -> bool:
        try:
            p = urlparse(url)
            return (p.scheme, p.netloc) == (self.parsed_base.scheme, self.parsed_base.netloc)
        except Exception:
            return False
    
    def _normalize_url(self, url: str) -> str:
        p = urlparse(url)
        return urlunparse(p._replace(fragment=""))
    
    def _mutate_param(self, url: str, param: str, new_value: str) -> str:
        p = urlparse(url)
        q = dict(parse_qsl(p.query, keep_blank_values=True))
        q[param] = new_value
        return urlunparse(p._replace(query=urlencode(q, doseq=True)))
    
    def _get_params(self, url: str) -> List[str]:
        p = urlparse(url)
        return [k for k, _ in parse_qsl(p.query, keep_blank_values=True)]
    
    def _get_param_value(self, url: str, param: str) -> str:
        p = urlparse(url)
        q = dict(parse_qsl(p.query, keep_blank_values=True))
        return q.get(param, "")
    
    # -------------------------------------------------------------------------
    # Crawling
    # -------------------------------------------------------------------------
    
    def extract_links(self, html: str, current_url: str) -> List[str]:
        if not html:
            return []
        
        links: List[str] = []
        
        if HAS_BS4:
            soup = BeautifulSoup(html, "html.parser")
            for a in soup.find_all("a", href=True):
                href = a.get("href", "")
                if href and not href.startswith(("javascript:", "mailto:", "tel:", "#")):
                    abs_url = urljoin(current_url, href)
                    links.append(self._normalize_url(abs_url))
            for form in soup.find_all("form", action=True):
                action = form.get("action", "")
                if action:
                    abs_url = urljoin(current_url, action)
                    links.append(self._normalize_url(abs_url))
        else:
            for m in re.finditer(r'(?:href|action)=["\']([^"\']+)["\']', html, re.IGNORECASE):
                href = m.group(1)
                if href and not href.startswith(("javascript:", "mailto:", "tel:", "#")):
                    abs_url = urljoin(current_url, href)
                    links.append(self._normalize_url(abs_url))
        
        return list(dict.fromkeys(u for u in links if self._same_host(u)))
    
    def crawl(self) -> Set[str]:
        """BFS crawl with progress display."""
        print(f"\n{Colors.YELLOW}[*] Starting crawl (max_depth={self.max_depth}, max_requests={self.max_requests})...{Colors.END}")
        
        discovered: Set[str] = set()
        queue: List[Tuple[str, int]] = [(self.base_url, 0)]
        seen: Set[str] = set()
        
        while queue and self._request_count < self.max_requests:
            url, depth = queue.pop(0)
            
            if url in seen:
                continue
            seen.add(url)
            
            resp = self._get(url)
            if resp is None:
                continue
            
            final_url = self._normalize_url(resp.url)
            discovered.add(final_url)
            self._stats.urls_crawled = len(discovered)
            
            # Progress indicator
            if len(discovered) % 10 == 0:
                print(f"  {Colors.CYAN}Crawled: {len(discovered)} URLs | Requests: {self._request_count}{Colors.END}", end='\r')
            
            if depth >= self.max_depth:
                continue
            
            content_type = resp.headers.get("Content-Type", "")
            if "text/html" not in content_type.lower():
                continue
            
            for link in self.extract_links(resp.text or "", resp.url):
                if link not in seen:
                    queue.append((link, depth + 1))
        
        print(f"\n  {Colors.GREEN}✓ Crawl complete: {len(discovered)} URLs discovered{Colors.END}")
        return discovered
    
    # -------------------------------------------------------------------------
    # Finding Management (Thread-safe)
    # -------------------------------------------------------------------------
    
    def _add_finding(self, finding: Finding) -> None:
        with self._findings_lock:
            self._findings.append(finding)
            self._stats.findings_count = len(self._findings)
            
            if "SQL" in finding.vuln_type:
                self._stats.sqli_findings += 1
            elif "XSS" in finding.vuln_type:
                self._stats.xss_findings += 1
            elif "Command Injection" in finding.vuln_type:
                self._stats.cmdi_findings += 1
            elif "Path Traversal" in finding.vuln_type or "LFI" in finding.vuln_type:
                self._stats.lfi_findings += 1
            elif "CORS" in finding.vuln_type:
                self._stats.cors_findings += 1
            elif "Cookie" in finding.vuln_type:
                self._stats.cookie_findings += 1
            elif "Technology" in finding.vuln_type or "Fingerprinting" in finding.vuln_type:
                self._stats.tech_findings += 1
            elif "Header" in finding.vuln_type:
                self._stats.header_findings += 1
            elif "Exposure" in finding.vuln_type:
                self._stats.exposure_findings += 1
        
        write_finding_jsonl(self.out_file, asdict(finding), self._log_lock)
    
    # -------------------------------------------------------------------------
    # SQL Injection Detection
    # -------------------------------------------------------------------------
    
    def _detect_db_error(self, body: str) -> Optional[str]:
        for pattern in self.DB_ERROR_PATTERNS:
            if pattern.search(body or ""):
                return pattern.pattern
        return None
    
    def scan_sqli_error_based(self, url: str) -> List[Finding]:
        findings: List[Finding] = []
        params = self._get_params(url)
        
        if not params:
            return findings
        
        baseline_resp = self._get(url)
        if baseline_resp is None:
            return findings
        baseline_body = baseline_resp.text or ""
        baseline_len = len(baseline_body)
        
        for param in params:
            original_value = self._get_param_value(url, param)
            
            for payload in self.SQLI_PAYLOADS_ERROR:
                if self._request_count >= self.max_requests:
                    return findings
                    
                test_value = f"{original_value}{payload}"
                test_url = self._mutate_param(url, param, test_value)
                
                test_resp = self._get(test_url)
                if test_resp is None:
                    continue
                
                test_body = test_resp.text or ""
                error_pattern = self._detect_db_error(test_body)
                
                len_delta = abs(len(test_body) - baseline_len)
                status_changed = test_resp.status_code != baseline_resp.status_code
                significant_change = status_changed or len_delta > max(200, int(0.15 * baseline_len))
                
                if error_pattern and significant_change:
                    confidence = Confidence.HIGH.value
                elif error_pattern:
                    confidence = Confidence.MEDIUM.value
                else:
                    continue
                
                severity = Severity.CRITICAL.value if confidence == Confidence.HIGH.value else Severity.HIGH.value
                
                finding = Finding(
                    timestamp=iso_now(),
                    vuln_type="SQL Injection (Error-Based)",
                    severity=severity,
                    confidence=confidence,
                    target_url=url,
                    parameter=param,
                    http_method="GET",
                    payload=payload,
                    evidence={
                        "test_url": test_url,
                        "db_error_pattern": error_pattern,
                        "status_code": test_resp.status_code,
                        "response_len_delta": len_delta,
                    },
                    mitigation=MitigationDatabase.get_short_mitigation("SQLi"),
                    cwe_id=MitigationDatabase.get_cwe("SQLi"),
                    owasp_category=MitigationDatabase.get_owasp("SQLi"),
                )
                
                findings.append(finding)
                self._add_finding(finding)
                
                print(f"  {Colors.RED}[!] SQLi FOUND: {url} | param={param} | {confidence}{Colors.END}")
                break
        
        return findings
    
    # -------------------------------------------------------------------------
    # XSS Detection
    # -------------------------------------------------------------------------
    
    def scan_xss_reflected(self, url: str) -> List[Finding]:
        findings: List[Finding] = []
        params = self._get_params(url)
        
        if not params:
            return findings
        
        for param in params:
            for payload in self.XSS_PAYLOADS:
                if self._request_count >= self.max_requests:
                    return findings
                    
                test_url = self._mutate_param(url, param, payload)
                
                test_resp = self._get(test_url)
                if test_resp is None:
                    continue
                
                test_body = test_resp.text or ""
                
                if self.XSS_DETECTION_MARKER in test_body or payload in test_body:
                    in_script = "<script>" in test_body.lower() and payload.lower() in test_body.lower()
                    confidence = Confidence.HIGH.value if in_script else Confidence.MEDIUM.value
                    
                    finding = Finding(
                        timestamp=iso_now(),
                        vuln_type="Cross-Site Scripting (Reflected XSS)",
                        severity=Severity.HIGH.value,
                        confidence=confidence,
                        target_url=url,
                        parameter=param,
                        http_method="GET",
                        payload=payload,
                        evidence={
                            "test_url": test_url,
                            "payload_reflected": True,
                            "in_script_context": in_script,
                        },
                        mitigation=MitigationDatabase.get_short_mitigation("XSS"),
                        cwe_id=MitigationDatabase.get_cwe("XSS"),
                        owasp_category=MitigationDatabase.get_owasp("XSS"),
                    )
                    
                    findings.append(finding)
                    self._add_finding(finding)
                    
                    print(f"  {Colors.RED}[!] XSS FOUND: {url} | param={param} | {confidence}{Colors.END}")
                    break
        
        return findings
    
    # -------------------------------------------------------------------------
    # Command Injection Detection
    # -------------------------------------------------------------------------
    
    def scan_cmdi(self, url: str) -> List[Finding]:
        findings: List[Finding] = []
        params = self._get_params(url)
        
        if not params:
            return findings
        
        for param in params:
            original_value = self._get_param_value(url, param)
            
            # Time-based detection
            for payload in self.CMDI_PAYLOADS:
                if self._request_count >= self.max_requests:
                    return findings
                    
                test_value = f"{original_value}{payload}"
                test_url = self._mutate_param(url, param, test_value)
                
                start = time.time()
                test_resp = self._get(test_url)
                elapsed = time.time() - start
                
                if test_resp is None:
                    continue
                
                # If response takes significantly longer (3+ seconds), likely vulnerable
                if elapsed >= 2.5:
                    finding = Finding(
                        timestamp=iso_now(),
                        vuln_type="Command Injection (Time-Based)",
                        severity=Severity.CRITICAL.value,
                        confidence=Confidence.HIGH.value,
                        target_url=url,
                        parameter=param,
                        http_method="GET",
                        payload=payload,
                        evidence={
                            "test_url": test_url,
                            "response_time": f"{elapsed:.2f}s",
                            "expected_delay": "3s",
                        },
                        mitigation=MitigationDatabase.get_short_mitigation("CMDI"),
                        cwe_id=MitigationDatabase.get_cwe("CMDI"),
                        owasp_category=MitigationDatabase.get_owasp("CMDI"),
                    )
                    
                    findings.append(finding)
                    self._add_finding(finding)
                    
                    print(f"  {Colors.RED}[!] CMDI FOUND: {url} | param={param} | {elapsed:.2f}s delay{Colors.END}")
                    return findings  # Stop after first positive
        
        return findings
    
    # -------------------------------------------------------------------------
    # Path Traversal / LFI Detection
    # -------------------------------------------------------------------------
    
    def scan_lfi(self, url: str) -> List[Finding]:
        findings: List[Finding] = []
        params = self._get_params(url)
        
        if not params:
            return findings
        
        for param in params:
            for payload in self.LFI_PAYLOADS:
                if self._request_count >= self.max_requests:
                    return findings
                    
                test_url = self._mutate_param(url, param, payload)
                
                test_resp = self._get(test_url)
                if test_resp is None:
                    continue
                
                test_body = test_resp.text or ""
                
                # Check for file inclusion signatures
                matched_signature = None
                for sig in self.LFI_SIGNATURES:
                    if sig.search(test_body):
                        matched_signature = sig.pattern
                        break
                
                if matched_signature:
                    finding = Finding(
                        timestamp=iso_now(),
                        vuln_type="Path Traversal / Local File Inclusion",
                        severity=Severity.CRITICAL.value,
                        confidence=Confidence.HIGH.value,
                        target_url=url,
                        parameter=param,
                        http_method="GET",
                        payload=payload,
                        evidence={
                            "test_url": test_url,
                            "signature_matched": matched_signature,
                            "content_preview": test_body[:200],
                        },
                        mitigation=MitigationDatabase.get_short_mitigation("LFI"),
                        cwe_id=MitigationDatabase.get_cwe("LFI"),
                        owasp_category=MitigationDatabase.get_owasp("LFI"),
                    )
                    
                    findings.append(finding)
                    self._add_finding(finding)
                    
                    print(f"  {Colors.RED}[!] LFI FOUND: {url} | param={param}{Colors.END}")
                    break
        
        return findings
    
    # -------------------------------------------------------------------------
    # CORS Misconfiguration Detection
    # -------------------------------------------------------------------------
    
    def scan_cors(self, url: str) -> List[Finding]:
        findings: List[Finding] = []
        
        malicious_origins = [
            "https://evil.com",
            "null",
            "https://attacker.com",
        ]
        
        for origin in malicious_origins:
            if self._request_count >= self.max_requests:
                break
            
            resp = self._get(url, headers={"Origin": origin})
            if resp is None:
                continue
            
            acao = resp.headers.get("Access-Control-Allow-Origin", "")
            acac = resp.headers.get("Access-Control-Allow-Credentials", "").lower()
            
            # Check for dangerous CORS configurations
            is_vulnerable = False
            issue_description = ""
            
            if acao == "*" and acac == "true":
                is_vulnerable = True
                issue_description = "Wildcard CORS with credentials enabled"
            elif acao == origin:
                is_vulnerable = True
                issue_description = f"Reflects untrusted origin: {origin}"
            elif acao == "null":
                is_vulnerable = True
                issue_description = "Accepts null origin"
            
            if is_vulnerable:
                finding = Finding(
                    timestamp=iso_now(),
                    vuln_type="CORS Misconfiguration",
                    severity=Severity.HIGH.value,
                    confidence=Confidence.HIGH.value,
                    target_url=url,
                    parameter="CORS Headers",
                    http_method="GET",
                    payload=f"Origin: {origin}",
                    evidence={
                        "issue": issue_description,
                        "Access-Control-Allow-Origin": acao,
                        "Access-Control-Allow-Credentials": acac,
                        "test_origin": origin,
                    },
                    mitigation=MitigationDatabase.get_short_mitigation("CORS"),
                    cwe_id=MitigationDatabase.get_cwe("CORS"),
                    owasp_category=MitigationDatabase.get_owasp("CORS"),
                )
                
                findings.append(finding)
                self._add_finding(finding)
                
                print(f"  {Colors.YELLOW}[!] CORS Issue: {issue_description}{Colors.END}")
                break  # Stop after first match
        
        return findings
    
    # -------------------------------------------------------------------------
    # Cookie Security Analysis
    # -------------------------------------------------------------------------
    
    def scan_cookie_security(self, url: str) -> List[Finding]:
        findings: List[Finding] = []
        
        resp = self._get(url)
        if resp is None:
            return findings
        
        cookies = resp.cookies
        if not cookies:
            return findings
        
        issues = []
        for cookie in cookies:
            cookie_issues = []
            
            if not cookie.secure:
                cookie_issues.append("Missing Secure flag")
            
            # Check for HttpOnly (requests library exposes this)
            if not cookie.has_nonstandard_attr("HttpOnly"):
                cookie_issues.append("Missing HttpOnly flag")
            
            # Check for SameSite
            if not cookie.has_nonstandard_attr("SameSite"):
                cookie_issues.append("Missing SameSite attribute")
            
            if cookie_issues:
                issues.append({
                    "cookie_name": cookie.name,
                    "issues": cookie_issues,
                    "domain": cookie.domain,
                    "path": cookie.path,
                })
        
        if issues:
            finding = Finding(
                timestamp=iso_now(),
                vuln_type="Insecure Cookie Configuration",
                severity=Severity.MEDIUM.value,
                confidence=Confidence.CONFIRMED.value,
                target_url=url,
                parameter="Set-Cookie Headers",
                http_method="GET",
                payload="N/A",
                evidence={
                    "insecure_cookies": issues,
                    "total_cookies": len(cookies),
                },
                mitigation=MitigationDatabase.get_short_mitigation("COOKIE_SECURITY"),
                cwe_id=MitigationDatabase.get_cwe("COOKIE_SECURITY"),
                owasp_category=MitigationDatabase.get_owasp("COOKIE_SECURITY"),
            )
            
            findings.append(finding)
            self._add_finding(finding)
            
            print(f"  {Colors.YELLOW}[i] Cookie security issues found: {len(issues)} cookie(s){Colors.END}")
        
        return findings
    
    # -------------------------------------------------------------------------
    # Technology Fingerprinting
    # -------------------------------------------------------------------------
    
    def scan_technology(self, url: str) -> List[Finding]:
        findings: List[Finding] = []
        
        resp = self._get(url)
        if resp is None:
            return findings
        
        detected_tech = {
            "servers": [],
            "frameworks": [],
            "cms": [],
        }
        
        # Check headers for server/framework info
        server_header = resp.headers.get("Server", "")
        powered_by = resp.headers.get("X-Powered-By", "")
        
        for category, patterns in self.TECH_SIGNATURES.items():
            for pattern, tech_name in patterns.items():
                # Check headers
                if pattern.search(server_header) or pattern.search(powered_by):
                    detected_tech[category].append(tech_name)
                # Check body for CMS signatures
                elif category == "cms" and pattern.search(resp.text or ""):
                    detected_tech[category].append(tech_name)
        
        # Only create finding if something was detected
        any_detected = any(detected_tech.values())
        if any_detected:
            finding = Finding(
                timestamp=iso_now(),
                vuln_type="Technology Fingerprinting",
                severity=Severity.INFO.value,
                confidence=Confidence.CONFIRMED.value,
                target_url=url,
                parameter="N/A",
                http_method="GET",
                payload="N/A",
                evidence={
                    "detected_technologies": detected_tech,
                    "server_header": server_header,
                    "powered_by": powered_by,
                },
                mitigation="Review technology disclosure; minimize information leakage.",
                cwe_id="CWE-200",
                owasp_category="A05:2021 - Security Misconfiguration",
            )
            
            findings.append(finding)
            self._add_finding(finding)
            
            tech_list = []
            for cat, techs in detected_tech.items():
                if techs:
                    tech_list.extend(techs)
            print(f"  {Colors.CYAN}[i] Technologies detected: {', '.join(tech_list)}{Colors.END}")
        
        return findings
    
    # -------------------------------------------------------------------------
    # Security Headers
    # -------------------------------------------------------------------------
    
    def scan_security_headers(self, url: str) -> List[Finding]:
        findings: List[Finding] = []
        
        resp = self._get(url)
        if resp is None:
            return findings
        
        headers_lower = {k.lower(): v for k, v in resp.headers.items()}
        missing_headers = []
        
        for header, (description, severity) in self.SECURITY_HEADERS.items():
            if header.lower() not in headers_lower:
                missing_headers.append({
                    "header": header,
                    "issue": description,
                    "severity": severity.value,
                })
        
        if missing_headers:
            finding = Finding(
                timestamp=iso_now(),
                vuln_type="Missing Security Headers",
                severity=Severity.MEDIUM.value,
                confidence=Confidence.CONFIRMED.value,
                target_url=url,
                parameter="N/A",
                http_method="GET",
                payload="N/A",
                evidence={
                    "missing_headers": missing_headers,
                    "present_headers": list(headers_lower.keys()),
                },
                mitigation=MitigationDatabase.get_short_mitigation("SECURITY_HEADERS"),
                cwe_id=MitigationDatabase.get_cwe("SECURITY_HEADERS"),
                owasp_category=MitigationDatabase.get_owasp("SECURITY_HEADERS"),
            )
            
            findings.append(finding)
            self._add_finding(finding)
            
            print(f"  {Colors.YELLOW}[i] Missing headers: {[h['header'] for h in missing_headers]}{Colors.END}")
        
        return findings
    
    # -------------------------------------------------------------------------
    # Response Classification & FP Handling Logic
    # -------------------------------------------------------------------------

    def _is_spa_routing(self, body: str) -> bool:
        """Heuristic to detect generic SPA catch-all pages."""
        if not body:
            return False
        
        # Common SPA/Modern framework markers
        spa_markers = [
            '<div id="root">', '<div id="app">', '<app-root>',
            'window.__NEXT_DATA__', 'window.__nuxt__',
            'react-root', '_next/static', '/static/js/main.'
        ]
        
        body_lower = body.lower()
        score = 0
        for marker in spa_markers:
            if marker.lower() in body_lower:
                score += 1
        
        # If we see 2+ markers, or it's very short HTML with a root div
        if score >= 1 and len(body) < 5000:
            return True
            
        return False

    def _classify_exposure(self, resp: requests.Response, path: str) -> Tuple[FindingStatus, str, str]:
        """
        Classify a potential sensitive file hit strict user-defined rules.
        Returns: (FindingStatus, Severity, BodyFingerprint)
        """
        status_code = resp.status_code
        body = (resp.text or "")
        content_type = resp.headers.get("Content-Type", "").lower().split(";")[0].strip()
        body_len = len(body)
        
        body_fingerprint = "unknown"
        if "<html" in body.lower():
            body_fingerprint = "html_page"
        elif "{" in body and "}" in body and ":" in body:
            body_fingerprint = "json_data"
        elif "password" in body.lower() or "secret" in body.lower() or "key" in body.lower():
            body_fingerprint = "potential_secrets"
        elif body_len > 0:
            body_fingerprint = "plain_text" if "text/plain" in content_type else "binary_data"

        # 1. NOT FOUND (Strict 404)
        if status_code == 404:
            return FindingStatus.NOT_FOUND, Severity.INFO.value, "not_found"

        # 2. Informational files (Always INFO if present)
        informational_paths = ["robots.txt", "security.txt", "humans.txt", ".well-known/security.txt"]
        if any(path.endswith(p) for p in informational_paths) and status_code == 200:
            return FindingStatus.INFORMATIONAL, Severity.INFO.value, "public_info_file"

        # 3. Handle Redirections (3xx)
        if resp.history or status_code in [301, 302, 307, 308]:
            final_url = resp.url.lower()
            if "login" in final_url or "signin" in final_url:
                return FindingStatus.REDIRECTED, Severity.INFO.value, "redirect_to_login"
            return FindingStatus.REDIRECTED, Severity.INFO.value, "redirected"

        # 4. Access Controlled (401/403)
        if status_code in [401, 403]:
            return FindingStatus.ACCESS_CONTROLLED, Severity.LOW.value, "access_denied"

        # 5. SPA / Generic Detection (200 OK but generic content)
        if self._is_spa_routing(body) or "404" in body[:500] or "not found" in body[:500].lower():
             return FindingStatus.NOT_EXPOSED, Severity.INFO.value, "spa_or_soft_404"

        # 6. Strict CONFIRMED Logic for 200 OK
        if status_code == 200:
            # Sensitive Keywords
            sensitive_indicators = [
                "password", "secret", "api_key", "private_key", "database",
                "mongodb://", "mysql://", "<?php", "AWS_", "PRIVATE KEY",
                "BEGIN RSA PRIVATE KEY", "DB_PASSWORD", "DB_USERNAME", "S3_BUCKET"
            ]
            has_sensitive_content = any(ind.lower() in body.lower() for ind in sensitive_indicators)
            
            # Binary / Config Extensions
            binary_exts = ['.zip', '.tar', '.gz', '.sql', '.db', '.sqlite', '.bak', '.old', '.env', '.config']
            is_binary_path = any(path.endswith(ext) for ext in binary_exts)
            
            # Content-Type Check
            is_valid_type = any(t in content_type for t in ["text/plain", "application/json", "application/x-yaml", "application/xml", "text/yaml", "application/octet-stream"])
            
            # RULE: 200 + Non-HTML + Matches Signature => EXPOSED
            if has_sensitive_content and (is_valid_type or "text/html" not in content_type):
                return FindingStatus.CONFIRMED, Severity.CRITICAL.value, "confirmed_secrets_found"
            
            # RULE: 200 + Binary Ext + Correct Type => EXPOSED
            if is_binary_path and (is_valid_type or "application" in content_type):
                return FindingStatus.CONFIRMED, Severity.HIGH.value, "confirmed_binary_or_backup"

            # RULE: 200 + Text/HTML (Generic) => NOT EXPOSED (Ignore)
            if "text/html" in content_type:
                return FindingStatus.NOT_EXPOSED, Severity.INFO.value, "html_ok_ignored"
                
            # Fallback: 200 OK but weird type -> Potentially interesting but low confidence
            return FindingStatus.ENUMERATED, Severity.INFO.value, "unverified_enumeration_hit"

        # Default fallback
        return FindingStatus.NOT_EXPOSED, Severity.INFO.value, "unknown_status"

    # -------------------------------------------------------------------------
    # Sensitive File Exposure
    # -------------------------------------------------------------------------
    
    def scan_sensitive_files(self) -> List[Finding]:
        findings: List[Finding] = []
        base = f"{self.parsed_base.scheme}://{self.parsed_base.netloc}"
        
        print(f"\n{Colors.YELLOW}[*] Checking for sensitive file exposure (strict verification)...{Colors.END}")
        
        for path in self.SENSITIVE_PATHS:
            if self._request_count >= self.max_requests:
                break
            
            test_url = urljoin(base, path)
            resp = self._get(test_url)
            
            if resp is None:
                continue
            
            # Use the new classification engine
            finding_status, severity, fingerprint = self._classify_exposure(resp, path)
            
            # 1. Handle NOT FOUND - Add to Appendix, do not report
            if finding_status == FindingStatus.NOT_FOUND:
                self._not_found_paths.append(f"{test_url} ({resp.status_code})")
                continue

            # 2. Handle NOT EXPOSED - Ignore completely (False Positives)
            if finding_status == FindingStatus.NOT_EXPOSED:
                continue

            # 3. Create Findings for interesting results
            # Taxonomy renaming based on user request:
            vuln_type = "Sensitive Path Probe"
            
            if finding_status == FindingStatus.CONFIRMED:
                vuln_type = "Sensitive File Exposure" # Reserved for confirmed hits
            elif finding_status == FindingStatus.ACCESS_CONTROLLED:
                vuln_type = "Access Controlled Resource"
            elif finding_status == FindingStatus.REDIRECTED:
                vuln_type = "Redirected Path"
            elif finding_status == FindingStatus.INFORMATIONAL:
                vuln_type = "Public Information"

            # Confidence Logic
            confidence = Confidence.LOW.value
            if finding_status == FindingStatus.CONFIRMED:
                confidence = Confidence.HIGH.value
            elif finding_status in [FindingStatus.ACCESS_CONTROLLED, FindingStatus.INFORMATIONAL]:
                confidence = Confidence.CONFIRMED.value # Status code is confirmed behavior
            
            finding = Finding(
                timestamp=iso_now(),
                vuln_type=vuln_type,
                severity=severity,
                confidence=confidence,
                target_url=test_url,
                parameter="N/A",
                http_method="GET",
                payload=path,
                evidence={
                    "status_code": resp.status_code,
                    "content_length": len(resp.text or ""),
                    "finding_status": finding_status.value,
                    "content_type": resp.headers.get("Content-Type", ""),
                    "redirect_history": [r.url for r in resp.history] if resp.history else [],
                    "body_fingerprint": fingerprint
                },
                mitigation=MitigationDatabase.get_short_mitigation("SENSITIVE_EXPOSURE"),
                cwe_id=MitigationDatabase.get_cwe("SENSITIVE_EXPOSURE"),
                owasp_category=MitigationDatabase.get_owasp("SENSITIVE_EXPOSURE"),
                finding_status=finding_status.value,
                http_status=resp.status_code,
                content_type=resp.headers.get("Content-Type", "").split(";")[0],
                final_url=resp.url,
                body_fingerprint=fingerprint
            )
            
            findings.append(finding)
            self._add_finding(finding)
            
            # Terminal UX
            color = Colors.RED if severity == Severity.CRITICAL.value or severity == Severity.HIGH.value else Colors.YELLOW
            status_label = finding_status.value.replace("_", " ")
            print(f"  {color}[!] {status_label}: {test_url} ({severity}){Colors.END}")
        
        return findings
    
    # -------------------------------------------------------------------------
    # Multi-threaded Scanning
    # -------------------------------------------------------------------------
    
    def _scan_url_task(self, url: str) -> List[Finding]:
        """Task for thread pool: scan a single URL for vulnerabilities."""
        findings = []
        
        if self._request_count >= self.max_requests:
            return findings
        
        if "?" in url:
            if self.enable_sqli:
                findings.extend(self.scan_sqli_error_based(url))
            if self.enable_xss:
                findings.extend(self.scan_xss_reflected(url))
            if self.enable_cmdi:
                findings.extend(self.scan_cmdi(url))
            if self.enable_lfi:
                findings.extend(self.scan_lfi(url))
        
        return findings
    
    # -------------------------------------------------------------------------
    # Main Scan Orchestration
    # -------------------------------------------------------------------------
    
    def scan(self, crawl: bool = True) -> List[Finding]:
        """Execute the full scan with multi-threading."""
        print(f"\n{Colors.BOLD}{Colors.GREEN}{'='*79}")
        print(f"                         STARTING VULNERABILITY SCAN")
        print(f"{'='*79}{Colors.END}")
        
        targets: Set[str] = {self.base_url}
        
        # Phase 0: Google Dorking (OSINT Reconnaissance)
        if self.enable_dorking:
            print(f"\n{Colors.YELLOW}[*] Running Google Dorks (OSINT)...{Colors.END}")
            print(f"  {Colors.CYAN}Note: This may take a while due to rate limiting{Colors.END}")
            
            # Extract domain from URL
            domain = self.parsed_base.netloc
            
            try:
                # Google Custom Search API credentials (100 free queries/day)
                API_KEY = "AIzaSyC3SB0fKNZ_TUjAu9LuuPfKgRmDP7Fq9ho"
                CSE_ID = "62a7d962af1bb4c57"
                
                dorker = GoogleDorker(
                    domain, 
                    max_results_per_dork=10,
                    api_key=API_KEY,
                    cse_id=CSE_ID
                )
                
                # Run limited dorks (to avoid excessive rate limiting)
                dork_categories = ['admin_panels', 'login_pages', 'subdomains']
                results = dorker.run_dorks(categories=dork_categories)
                
                # Print results
                dorker.print_results()
                
                # Add discovered URLs to targets
                discovered_urls = dorker.get_all_urls()
                if discovered_urls:
                    # Filter to only same domain
                    for url in discovered_urls:
                        if domain in url:
                            targets.add(url)
                    
                    self._stats.dorking_urls_found = len(discovered_urls)
                    print(f"\n  {Colors.GREEN}[+] Added {len(discovered_urls)} URLs from dorking to scan targets{Colors.END}")
            
            except Exception as e:
                print(f"  {Colors.RED}[!] Dorking error: {str(e)}{Colors.END}")
        
        # Phase 1: Crawling
        if crawl:
            targets |= self.crawl()
        
        # Phase 2: Technology Fingerprinting (single URL)
        if self.enable_tech:
            print(f"\n{Colors.YELLOW}[*] Fingerprinting technologies...{Colors.END}")
            self.scan_technology(self.base_url)
        
        # Phase 3: Security Headers (single URL)
        if self.enable_headers:
            print(f"\n{Colors.YELLOW}[*] Checking security headers...{Colors.END}")
            self.scan_security_headers(self.base_url)
        
        # Phase 4: CORS Misconfiguration (single URL)
        if self.enable_cors:
            print(f"\n{Colors.YELLOW}[*] Checking CORS configuration...{Colors.END}")
            self.scan_cors(self.base_url)
        
        # Phase 5: Cookie Security (single URL)
        if self.enable_cookies:
            print(f"\n{Colors.YELLOW}[*] Analyzing cookie security...{Colors.END}")
            self.scan_cookie_security(self.base_url)
        
        # Phase 6: Sensitive File Exposure
        if self.enable_exposure:
            self.scan_sensitive_files()
        
        # Phase 7: SQLi, XSS, CMDI, LFI with multi-threading
        urls_with_params = [u for u in sorted(targets) if "?" in u]
        
        if urls_with_params and (self.enable_sqli or self.enable_xss or self.enable_cmdi or self.enable_lfi):
            print(f"\n{Colors.YELLOW}[*] Scanning {len(urls_with_params)} URLs with parameters ({self.threads} threads)...{Colors.END}")
            
            with ThreadPoolExecutor(max_workers=self.threads) as executor:
                futures = {executor.submit(self._scan_url_task, url): url for url in urls_with_params}
                
                completed = 0
                for future in as_completed(futures):
                    completed += 1
                    if completed % 5 == 0:
                        print(f"  {Colors.CYAN}Progress: {completed}/{len(urls_with_params)} URLs scanned{Colors.END}", end='\r')
                    
                    if self._request_count >= self.max_requests:
                        print(f"\n  {Colors.YELLOW}Max requests reached, stopping scan...{Colors.END}")
                        break
            
            print(f"\n  {Colors.GREEN}✓ URL scanning complete{Colors.END}")
        
        # Finalize
        self._stats.end_time = iso_now()
        
        print(f"\n{Colors.BOLD}{Colors.GREEN}{'='*79}")
        print(f"                              SCAN COMPLETE")
        print(f"{'='*79}{Colors.END}")
        
        return self._findings
    
    def print_summary(self) -> None:
        """Print formatted summary."""
        print(f"\n{Colors.BOLD}{Colors.CYAN}{'='*79}")
        print(f"                              SCAN SUMMARY")
        print(f"{'='*79}{Colors.END}")
        print(f"  {Colors.GREEN}Target:{Colors.END}           {self.base_url}")
        print(f"  {Colors.GREEN}Duration:{Colors.END}         {self._stats.start_time} → {self._stats.end_time}")
        print(f"  {Colors.GREEN}URLs Crawled:{Colors.END}     {self._stats.urls_crawled}")
        print(f"  {Colors.GREEN}Requests Made:{Colors.END}    {self._stats.requests_made}")
        if self._stats.dorking_urls_found > 0:
            print(f"  {Colors.GREEN}Dorking URLs:{Colors.END}     {self._stats.dorking_urls_found}")
        print(f"  {Colors.GREEN}Output File:{Colors.END}      {self.out_file}")
        print(f"\n{Colors.BOLD}  FINDINGS BY TYPE:{Colors.END}")
        print(f"  🔴 SQL Injection:       {self._stats.sqli_findings}")
        print(f"  🟠 XSS:                 {self._stats.xss_findings}")
        print(f"  🔴 Command Injection:   {self._stats.cmdi_findings}")
        print(f"  🔴 Path Traversal/LFI:  {self._stats.lfi_findings}")
        print(f"  🟠 CORS Issues:         {self._stats.cors_findings}")
        print(f"  🟡 Cookie Security:     {self._stats.cookie_findings}")
        print(f"  🔵 Technology Info:     {self._stats.tech_findings}")
        print(f"  🟡 Security Headers:    {self._stats.header_findings}")
        print(f"  🟣 File Exposure:       {self._stats.exposure_findings}")
        print(f"  {'─'*40}")
        print(f"  {Colors.BOLD}📊 TOTAL FINDINGS:      {self._stats.findings_count}{Colors.END}")
        print(f"{Colors.CYAN}{'='*79}{Colors.END}")
        
        if self._findings:
            print(f"\n{Colors.BOLD}  DETAILED FINDINGS:{Colors.END}")
            for i, f in enumerate(self._findings, 1):
                severity_icon = {"CRITICAL": "🔴", "HIGH": "🟠", "MEDIUM": "🟡", "LOW": "🟢", "INFO": "⚪"}
                icon = severity_icon.get(f.severity, "⚪")
                print(f"\n  [{i}] {icon} {f.severity} | {f.vuln_type}")
                print(f"      URL:       {f.target_url}")
                print(f"      Parameter: {f.parameter}")
                payload_display = f.payload[:50] + "..." if len(f.payload) > 50 else f.payload
                print(f"      Payload:   {payload_display}")
                print(f"      CWE:       {f.cwe_id}")
        
        print(f"\n{Colors.CYAN}{'='*79}{Colors.END}")
    
    # -------------------------------------------------------------------------
    # Report Export Functions
    # -------------------------------------------------------------------------
    
    def export_json(self, filename: str = None) -> str:
        """Export findings and statistics to JSON format."""
        if filename is None:
            filename = self.out_file.replace('.jsonl', '_report.json')
        
        report = {
            "scan_info": {
                "target": self.base_url,
                "start_time": self._stats.start_time,
                "end_time": self._stats.end_time,
                "scanner_version": "4.0",
            },
            "statistics": asdict(self._stats),
            "findings": [asdict(f) for f in self._findings],
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\n{Colors.GREEN}[+] JSON report saved to: {filename}{Colors.END}")
        return filename
    
    def export_txt(self, filename: str = None) -> str:
        """Export findings to plain text format."""
        if filename is None:
            filename = self.out_file.replace('.jsonl', '_report.txt')
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("=" * 79 + "\n")
            f.write("     PENTEST AGENT v4.0 - SECURITY SCAN REPORT\n")
            f.write("=" * 79 + "\n\n")
            
            f.write(f"Target:         {self.base_url}\n")
            f.write(f"Start Time:     {self._stats.start_time}\n")
            f.write(f"End Time:       {self._stats.end_time}\n")
            f.write(f"URLs Crawled:   {self._stats.urls_crawled}\n")
            f.write(f"Requests Made:  {self._stats.requests_made}\n")
            f.write("\n" + "=" * 79 + "\n")
            f.write("FINDINGS SUMMARY\n")
            f.write("=" * 79 + "\n\n")
            
            f.write(f"SQL Injection:          {self._stats.sqli_findings:3d}\n")
            f.write(f"XSS:                    {self._stats.xss_findings:3d}\n")
            f.write(f"Command Injection:      {self._stats.cmdi_findings:3d}\n")
            f.write(f"Path Traversal/LFI:     {self._stats.lfi_findings:3d}\n")
            f.write(f"CORS Issues:            {self._stats.cors_findings:3d}\n")
            f.write(f"Cookie Security:        {self._stats.cookie_findings:3d}\n")
            f.write(f"Technology Info:        {self._stats.tech_findings:3d}\n")
            f.write(f"Security Headers:       {self._stats.header_findings:3d}\n")
            f.write(f"File Exposure:          {self._stats.exposure_findings:3d}\n")
            f.write("-" * 79 + "\n")
            f.write(f"TOTAL:                  {self._stats.findings_count:3d}\n\n")
            
            if self._findings:
                f.write("=" * 79 + "\n")
                f.write("DETAILED FINDINGS\n")
                f.write("=" * 79 + "\n\n")
                
                for i, finding in enumerate(self._findings, 1):
                    f.write(f"[{i}] {finding.severity} - {finding.vuln_type}\n")
                    f.write(f"    URL:       {finding.target_url}\n")
                    f.write(f"    Parameter: {finding.parameter}\n")
                    f.write(f"    Payload:   {finding.payload[:100]}\n")
                    f.write(f"    CWE:       {finding.cwe_id}\n")
                    f.write(f"    OWASP:     {finding.owasp_category}\n")
                    f.write(f"    Remediation: {finding.mitigation}\n")
                    f.write("\n" + "-" * 79 + "\n\n")

            if self._not_found_paths:
                f.write("=" * 79 + "\n")
                f.write(f"APPENDIX: ENUMERATED PATHS (NOT FOUND - {len(self._not_found_paths)})\n")
                f.write("=" * 79 + "\n\n")
                for path in self._not_found_paths:
                    f.write(f"    • {path}\n")
                f.write("\n")
        
        print(f"\n{Colors.GREEN}[+] TXT report saved to: {filename}{Colors.END}")
        return filename
    
    def export_html(self, filename: str = None) -> str:
        """Export findings to HTML format with styling."""
        if filename is None:
            filename = self.out_file.replace('.jsonl', '_report.html')
        
        severity_colors = {
            "CRITICAL": "#dc2626",
            "HIGH": "#f59e0b",
            "MEDIUM": "#eab308",
            "LOW": "#22c55e",
            "INFO": "#6b7280",
        }
        
        html = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Pentest Report - {self.base_url}</title>
    <style>
        * {{ margin: 0; padding: 0; box-sizing: border-box; }}
        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
            color: #333;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #1e3a8a 0%, #3b82f6 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }}
        .header h1 {{ font-size: 2.5em; margin-bottom: 10px; }}
        .header p {{ opacity: 0.9; font-size: 1.1em; }}
        .content {{ padding: 40px; }}
        .section {{
            margin-bottom: 40px;
            background: #f9fafb;
            padding: 25px;
            border-radius: 10px;
            border-left: 4px solid #3b82f6;
        }}
        .section h2 {{
            color: #1e40af;
            margin-bottom: 20px;
            font-size: 1.8em;
        }}
        .info-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
        }}
        .info-item {{
            background: white;
            padding: 15px;
            border-radius: 8px;
            border: 1px solid #e5e7eb;
        }}
        .info-label {{ font-weight: bold; color: #6b7280; margin-bottom: 5px; }}
        .info-value {{ font-size: 1.2em; color: #1f2937; }}
        .stats-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
        }}
        .stat-card {{
            background: white;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
            border: 2px solid #e5e7eb;
            transition: transform 0.2s;
        }}
        .stat-card:hover {{ transform: translateY(-5px); box-shadow: 0 4px 12px rgba(0,0,0,0.1); }}
        .stat-number {{ font-size: 2.5em; font-weight: bold; color: #3b82f6; }}
        .stat-label {{ color: #6b7280; margin-top: 5px; }}
        .finding {{
            background: white;
            padding: 20px;
            margin: 15px 0;
            border-radius: 8px;
            border-left: 5px solid;
        }}
        .critical {{ border-left-color: {severity_colors['CRITICAL']}; }}
        .high {{ border-left-color: {severity_colors['HIGH']}; }}
        .medium {{ border-left-color: {severity_colors['MEDIUM']}; }}
        .low {{ border-left-color: {severity_colors['LOW']}; }}
        .info {{ border-left-color: {severity_colors['INFO']}; }}
        .finding-header {{
            display: flex;
            align-items: center;
            margin-bottom: 15px;
        }}
        .severity-badge {{
            padding: 5px 15px;
            border-radius: 20px;
            color: white;
            font-weight: bold;
            margin-right: 15px;
            font-size: 0.85em;
        }}
        .finding-title {{ font-size: 1.3em; font-weight: bold; color: #1f2937; }}
        .finding-detail {{
            margin: 10px 0;
            padding: 10px;
            background: #f9fafb;
            border-radius: 5px;
        }}
        .detail-label {{ font-weight: bold; color: #6b7280; display: inline-block; width: 120px; }}
        .detail-value {{ color: #1f2937; }}
        code {{
            background: #1f2937;
            color: #10b981;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }}
        .footer {{
            background: #f3f4f6;
            padding: 20px;
            text-align: center;
            color: #6b7280;
            border-top: 1px solid #e5e7eb;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>🛡️ Security Scan Report</h1>
            <p>Pentest Agent v4.0 - Comprehensive Vulnerability Assessment</p>
        </div>
        
        <div class="content">
            <div class="section">
                <h2>📋 Scan Information</h2>
                <div class="info-grid">
                    <div class="info-item">
                        <div class="info-label">Target</div>
                        <div class="info-value">{self.base_url}</div>
                    </div>
                    <div class="info-item">
                        <div class="info-label">Start Time</div>
                        <div class="info-value">{self._stats.start_time[:19]}</div>
                    </div>
                    <div class="info-item">
                        <div class="info-label">End Time</div>
                        <div class="info-value">{self._stats.end_time[:19]}</div>
                    </div>
                    <div class="info-item">
                        <div class="info-label">URLs Crawled</div>
                        <div class="info-value">{self._stats.urls_crawled}</div>
                    </div>
                    <div class="info-item">
                        <div class="info-label">Total Requests</div>
                        <div class="info-value">{self._stats.requests_made}</div>
                    </div>
                    <div class="info-item">
                        <div class="info-label">Total Findings</div>
                        <div class="info-value">{self._stats.findings_count}</div>
                    </div>
                </div>
            </div>
            
            <div class="section">
                <h2>📊 Vulnerability Statistics</h2>
                <div class="stats-grid">
                    <div class="stat-card">
                        <div class="stat-number">{self._stats.sqli_findings}</div>
                        <div class="stat-label">SQL Injection</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">{self._stats.xss_findings}</div>
                        <div class="stat-label">XSS</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">{self._stats.cmdi_findings}</div>
                        <div class="stat-label">Command Injection</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">{self._stats.lfi_findings}</div>
                        <div class="stat-label">Path Traversal/LFI</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">{self._stats.cors_findings}</div>
                        <div class="stat-label">CORS Issues</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">{self._stats.cookie_findings}</div>
                        <div class="stat-label">Cookie Security</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">{self._stats.tech_findings}</div>
                        <div class="stat-label">Tech Fingerprints</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">{self._stats.header_findings}</div>
                        <div class="stat-label">Security Headers</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">{self._stats.exposure_findings}</div>
                        <div class="stat-label">File Exposure</div>
                    </div>
                </div>
            </div>"""
        
        if self._findings:
            html += """
            <div class="section">
                <h2>🔍 Detailed Findings</h2>
"""
            
            for i, finding in enumerate(self._findings, 1):
                severity_class = finding.severity.lower()
                severity_color = severity_colors.get(finding.severity, "#6b7280")
                
                html += f"""
                <div class="finding {severity_class}">
                    <div class="finding-header">
                        <span class="severity-badge" style="background: {severity_color};">{finding.severity}</span>
                        <span class="finding-title">{finding.vuln_type}</span>
                    </div>
                    <div class="finding-detail">
                        <span class="detail-label">URL:</span>
                        <span class="detail-value"><code>{finding.target_url}</code></span>
                    </div>
                    <div class="finding-detail">
                        <span class="detail-label">Parameter:</span>
                        <span class="detail-value"><code>{finding.parameter}</code></span>
                    </div>
                    <div class="finding-detail">
                        <span class="detail-label">Payload:</span>
                        <span class="detail-value"><code>{finding.payload[:100]}</code></span>
                    </div>
                    <div class="finding-detail">
                        <span class="detail-label">CWE:</span>
                        <span class="detail-value">{finding.cwe_id}</span>
                    </div>
                    <div class="finding-detail">
                        <span class="detail-label">OWASP:</span>
                        <span class="detail-value">{finding.owasp_category}</span>
                    </div>
                    <div class="finding-detail">
                        <span class="detail-label">Remediation:</span>
                        <span class="detail-value">{finding.mitigation}</span>
                    </div>
                </div>
"""
            
            html += "</div>"

        # Appendix Section (Not Found Paths)
        if self._not_found_paths:
            html += f"""
            <div class="section">
                <h2>📂 Appendix: Enumerated Paths (Not Found)</h2>
                <div class="info-grid" style="grid-template-columns: 1fr; background: #f3f4f6; padding: 15px; border-radius: 8px;">
                    <p style="margin-bottom: 10px; font-weight: bold;">The following paths were probed but returned 404 Not Found. They are listed here for completeness but are NOT considered vulnerabilities.</p>
                    <ul style="list-style-type: none; padding-left: 0;">
            """
            for path in self._not_found_paths:
                html += f'<li style="padding: 5px 0; border-bottom: 1px solid #e5e7eb; font-family: monospace;">{path}</li>'
            
            html += """
                    </ul>
                </div>
            </div>
            """
        
        html += f"""
        </div>
        
        <div class="footer">
            <p>Generated by <strong>Pentest Agent v4.0</strong> | {dt.datetime.now(dt.timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}</p>
        </div>
    </div>
</body>
</html>
"""
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(html)
        
        print(f"\n{Colors.GREEN}[+] HTML report saved to: {filename}{Colors.END}")
        return filename


# ==============================================================================
# INTERACTIVE CLI
# ==============================================================================

def get_target() -> str:
    """Get target URL/IP from user."""
    print(f"\n{Colors.BOLD}Enter target URL or IP address:{Colors.END}")
    print(f"{Colors.CYAN}  Examples: https://example.com, http://192.168.1.100:8080, example.com{Colors.END}")
    target = input(f"\n{Colors.GREEN}Target: {Colors.END}").strip()
    
    if not target:
        print(f"{Colors.RED}Error: Target cannot be empty!{Colors.END}")
        return get_target()
    
    return target


def get_custom_options() -> dict:
    """Interactive configuration for custom scan."""
    print(f"\n{Colors.BOLD}{Colors.HEADER}═══════════════════════════════════════════════════════════════════════════════")
    print(f"                           CUSTOM SCAN OPTIONS")
    print(f"═══════════════════════════════════════════════════════════════════════════════{Colors.END}\n")
    
    options = {}
    
    # Modules
    print(f"{Colors.BOLD}Select modules to enable (y/n):{Colors.END}")
    
    print(f"\n  {Colors.CYAN}SQL Injection Scanner{Colors.END}")
    print(f"  └─ Detects error-based and time-based SQL injection vulnerabilities")
    options['enable_sqli'] = input(f"     Enable? [Y/n]: ").strip().lower() != 'n'
    
    print(f"\n  {Colors.CYAN}XSS Scanner{Colors.END}")
    print(f"  └─ Detects reflected Cross-Site Scripting vulnerabilities")
    options['enable_xss'] = input(f"     Enable? [Y/n]: ").strip().lower() != 'n'
    
    print(f"\n  {Colors.CYAN}Security Headers Checker{Colors.END}")
    print(f"  └─ Analyzes HTTP security headers (HSTS, CSP, X-Frame-Options, etc.)")
    options['enable_headers'] = input(f"     Enable? [Y/n]: ").strip().lower() != 'n'
    
    print(f"\n  {Colors.CYAN}Sensitive File Exposure{Colors.END}")
    print(f"  └─ Checks for exposed config files, backups, admin panels, etc.")
    options['enable_exposure'] = input(f"     Enable? [Y/n]: ").strip().lower() != 'n'
    
    print(f"\n  {Colors.CYAN}Command Injection Scanner{Colors.END}")
    print(f"  └─ Detects OS command injection vulnerabilities")
    options['enable_cmdi'] = input(f"     Enable? [Y/n]: ").strip().lower() != 'n'
    
    print(f"\n  {Colors.CYAN}Path Traversal / LFI Scanner{Colors.END}")
    print(f"  └─ Detects file inclusion and path traversal vulnerabilities")
    options['enable_lfi'] = input(f"     Enable? [Y/n]: ").strip().lower() != 'n'
    
    print(f"\n  {Colors.CYAN}CORS Misconfiguration{Colors.END}")
    print(f"  └─ Checks for insecure cross-origin resource sharing policies")
    options['enable_cors'] = input(f"     Enable? [Y/n]: ").strip().lower() != 'n'
    
    print(f"\n  {Colors.CYAN}Cookie Security Analysis{Colors.END}")
    print(f"  └─ Analyzes cookie security flags (HttpOnly, Secure, SameSite)")
    options['enable_cookies'] = input(f"     Enable? [Y/n]: ").strip().lower() != 'n'
    
    print(f"\n  {Colors.CYAN}Technology Fingerprinting{Colors.END}")
    print(f"  └─ Identifies web servers, frameworks, and CMS platforms")
    options['enable_tech'] = input(f"     Enable? [Y/n]: ").strip().lower() != 'n'
    
    print(f"\n  {Colors.CYAN}Google Dorking (OSINT){Colors.END}")
    print(f"  └─ Reconnaissance using advanced Google search")
    print(f"  └─ {Colors.YELLOW}Warning: Slow (3-5s delays), may hit rate limits{Colors.END}")
    options['enable_dorking'] = input(f"     Enable? [y/N]: ").strip().lower() == 'y'
    
    if options['enable_sqli']:
        print(f"\n  {Colors.CYAN}Time-Based Blind SQLi{Colors.END}")
        print(f"  └─ Slower but detects blind SQL injection using time delays")
        print(f"  └─ {Colors.YELLOW}Warning: This is more intrusive and slower{Colors.END}")
        options['time_based_sqli'] = input(f"     Enable? [y/N]: ").strip().lower() == 'y'
    else:
        options['time_based_sqli'] = False
    
    # Performance settings
    print(f"\n{Colors.BOLD}Performance settings:{Colors.END}")
    
    print(f"\n  {Colors.CYAN}Max Requests{Colors.END}")
    print(f"  └─ Maximum number of HTTP requests to make (default: {DEFAULT_MAX_REQUESTS})")
    max_req = input(f"     Value [{DEFAULT_MAX_REQUESTS}]: ").strip()
    options['max_requests'] = int(max_req) if max_req.isdigit() else DEFAULT_MAX_REQUESTS
    
    print(f"\n  {Colors.CYAN}Thread Count{Colors.END}")
    print(f"  └─ Number of concurrent scanning threads (default: {DEFAULT_THREADS})")
    threads = input(f"     Value [{DEFAULT_THREADS}]: ").strip()
    options['threads'] = int(threads) if threads.isdigit() else DEFAULT_THREADS
    
    print(f"\n  {Colors.CYAN}Crawl Depth{Colors.END}")
    print(f"  └─ How many levels deep to crawl links (default: 3)")
    depth = input(f"     Value [3]: ").strip()
    options['max_depth'] = int(depth) if depth.isdigit() else 3
    
    print(f"\n  {Colors.CYAN}Request Delay{Colors.END}")
    print(f"  └─ Seconds between requests to avoid rate limiting (default: {DEFAULT_DELAY})")
    delay = input(f"     Value [{DEFAULT_DELAY}]: ").strip()
    try:
        options['delay_s'] = float(delay) if delay else DEFAULT_DELAY
    except ValueError:
        options['delay_s'] = DEFAULT_DELAY
    
    print(f"\n  {Colors.CYAN}Output File{Colors.END}")
    print(f"  └─ JSONL file to save findings (default: {DEFAULT_LOG_FILE})")
    out_file = input(f"     Filename [{DEFAULT_LOG_FILE}]: ").strip()
    options['out_file'] = out_file if out_file else DEFAULT_LOG_FILE
    
    return options


def run_interactive() -> None:
    """Main interactive CLI loop."""
    print(BANNER)
    
    while True:
        choice = input(MENU).strip()
        
        if choice == '0':
            print(f"\n{Colors.GREEN}Goodbye!{Colors.END}\n")
            sys.exit(0)
        
        if choice == '9':
            print(f"\n{Colors.CYAN}Use command line mode:{Colors.END}")
            print(f"  python pentest_agent.py --target <URL> [options]")
            print(f"  python pentest_agent.py --help")
            continue
        
        if choice not in ['1', '2', '3', '4', '5', '6', '7', '8']:
            print(f"{Colors.RED}Invalid choice. Please try again.{Colors.END}")
            continue
        
        # Get target
        target = get_target()
        
        # Configure based on choice
        if choice == '1':  # Quick Scan
            options = {
                'enable_sqli': True,
                'enable_xss': True,
                'enable_cmdi': True,
                'enable_lfi': True,
                'enable_cors': True,
                'enable_cookies': True,
                'enable_tech': True,
                'enable_headers': True,
                'enable_exposure': True,
                'time_based_sqli': False,
                'max_requests': 1000,
                'threads': 10,
                'max_depth': 2,
                'delay_s': 0.1,
                'out_file': DEFAULT_LOG_FILE,
            }
        elif choice == '2':  # Full Scan
            options = {
                'enable_sqli': True,
                'enable_xss': True,
                'enable_cmdi': True,
                'enable_lfi': True,
                'enable_cors': True,
                'enable_cookies': True,
                'enable_tech': True,
                'enable_headers': True,
                'enable_exposure': True,
                'enable_dorking': True,
                'time_based_sqli': True,
                'max_requests': 5000,
                'threads': 15,
                'max_depth': 4,
                'delay_s': 0.05,
                'out_file': DEFAULT_LOG_FILE,
            }
        elif choice == '3':  # Custom Scan
            options = get_custom_options()
        elif choice == '4':  # SQLi Only
            options = {
                'enable_sqli': True,
                'enable_xss': False,
                'enable_cmdi': False,
                'enable_lfi': False,
                'enable_cors': False,
                'enable_cookies': False,
                'enable_tech': False,
                'enable_headers': False,
                'enable_exposure': False,
                'time_based_sqli': False,
                'max_requests': 2000,
                'threads': 10,
                'max_depth': 3,
                'delay_s': 0.1,
                'out_file': DEFAULT_LOG_FILE,
            }
        elif choice == '5':  # XSS Only
            options = {
                'enable_sqli': False,
                'enable_xss': True,
                'enable_cmdi': False,
                'enable_lfi': False,
                'enable_cors': False,
                'enable_cookies': False,
                'enable_tech': False,
                'enable_headers': False,
                'enable_exposure': False,
                'time_based_sqli': False,
                'max_requests': 2000,
                'threads': 10,
                'max_depth': 3,
                'delay_s': 0.1,
                'out_file': DEFAULT_LOG_FILE,
            }
        elif choice == '6':  # Headers Only
            options = {
                'enable_sqli': False,
                'enable_xss': False,
                'enable_cmdi': False,
                'enable_lfi': False,
                'enable_cors': False,
                'enable_cookies': False,
                'enable_tech': False,
                'enable_headers': True,
                'enable_exposure': False,
                'time_based_sqli': False,
                'max_requests': 100,
                'threads': 1,
                'max_depth': 0,
                'delay_s': 0,
                'out_file': DEFAULT_LOG_FILE,
            }
        elif choice == '7':  # File Exposure Only
            options = {
                'enable_sqli': False,
                'enable_xss': False,
                'enable_cmdi': False,
                'enable_lfi': False,
                'enable_cors': False,
                'enable_cookies': False,
                'enable_tech': False,
                'enable_headers': False,
                'enable_exposure': True,
                'time_based_sqli': False,
                'max_requests': 100,
                'threads': 5,
                'max_depth': 0,
                'delay_s': 0.1,
                'out_file': DEFAULT_LOG_FILE,
            }
        elif choice == '8':  # Google Dorking Only
            options = {
                'enable_sqli': False,
                'enable_xss': False,
                'enable_cmdi': False,
                'enable_lfi': False,
                'enable_cors': False,
                'enable_cookies': False,
                'enable_tech': False,
                'enable_headers': False,
                'enable_exposure': False,
                'enable_dorking': True,
                'time_based_sqli': False,
                'max_requests': 50,  # Dorking doesn't use many requests
                'threads': 1,
                'max_depth': 0,
                'delay_s': 3,  # Slower to avoid Google rate limits
                'out_file': DEFAULT_LOG_FILE,
            }
        
        # Confirmation
        print(f"\n{Colors.YELLOW}Ready to scan {target}?{Colors.END}")
        confirm = input(f"Press Enter to start or 'q' to cancel: ").strip().lower()
        
        if confirm == 'q':
            continue
        
        # Run scan
        agent = PentestAgent(
            target,
            **options
        )
        
        try:
            agent.scan(crawl=options.get('max_depth', 3) > 0)
            agent.print_summary()
            
            # Ask about report export
            print(f"\n{Colors.CYAN}{'─'*79}{Colors.END}")
            print(f"\n{Colors.BOLD}Would you like to export the report?{Colors.END}")
            print(f"{Colors.CYAN}  [1]{Colors.END} JSON format (machine-readable)")
            print(f"{Colors.CYAN}  [2]{Colors.END} TXT format (plain text)")
            print(f"{Colors.CYAN}  [3]{Colors.END} HTML format (professional report)")
            print(f"{Colors.CYAN}  [4]{Colors.END} All formats")
            print(f"{Colors.CYAN}  [0]{Colors.END} Skip")
            
            export_choice = input(f"\n{Colors.GREEN}Choose format: {Colors.END}").strip()
            
            if export_choice == '1':
                agent.export_json()
            elif export_choice == '2':
                agent.export_txt()
            elif export_choice == '3':
                agent.export_html()
            elif export_choice == '4':
                agent.export_json()
                agent.export_txt()
                agent.export_html()
            
        except KeyboardInterrupt:
            print(f"\n{Colors.YELLOW}Scan interrupted by user.{Colors.END}")
            agent.print_summary()
        
        # Ask if user wants to continue
        print(f"\n{Colors.CYAN}{'─'*79}{Colors.END}")
        again = input(f"\nWould you like to run another scan? [y/N]: ").strip().lower()
        if again != 'y':
            print(f"\n{Colors.GREEN}Thank you for using Pentest Agent!{Colors.END}\n")
            break


# ==============================================================================
# CLI ARGUMENT PARSER (for traditional command-line usage)
# ==============================================================================

def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Autonomous Pentest Agent - Interactive & Multi-threaded Vulnerability Scanner",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Interactive mode (recommended)
  python pentest_agent.py

  # Command line mode
  python pentest_agent.py --target http://example.com
  python pentest_agent.py --target http://192.168.1.100:8080 --sqli-only --threads 20
  python pentest_agent.py --target https://target.com --full-scan
        """
    )
    
    parser.add_argument("--target", "-t", help="Target URL (skip interactive mode)")
    parser.add_argument("--no-crawl", action="store_true", help="Disable crawling")
    parser.add_argument("--max-depth", type=int, default=3, help="Max crawl depth (default: 3)")
    parser.add_argument("--sqli-only", action="store_true", help="Only SQL injection tests")
    parser.add_argument("--xss-only", action="store_true", help="Only XSS tests")
    parser.add_argument("--headers-only", action="store_true", help="Only security headers")
    parser.add_argument("--no-sqli", action="store_true", help="Disable SQLi tests")
    parser.add_argument("--no-xss", action="store_true", help="Disable XSS tests")
    parser.add_argument("--no-cmdi", action="store_true", help="Disable Command Injection tests")
    parser.add_argument("--no-lfi", action="store_true", help="Disable Path Traversal/LFI tests")
    parser.add_argument("--no-cors", action="store_true", help="Disable CORS checks")
    parser.add_argument("--no-cookies", action="store_true", help="Disable Cookie Security checks")
    parser.add_argument("--no-tech", action="store_true", help="Disable Technology Fingerprinting")
    parser.add_argument("--no-headers", action="store_true", help="Disable headers check")
    parser.add_argument("--no-exposure", action="store_true", help="Disable file exposure check")
    parser.add_argument("--time-based", action="store_true", help="Enable time-based blind SQLi")
    parser.add_argument("--full-scan", action="store_true", help="Full scan with all options")
    parser.add_argument("--verify-tls", action="store_true", help="Verify TLS certificates")
    parser.add_argument("--timeout", type=int, default=DEFAULT_TIMEOUT, help=f"HTTP timeout (default: {DEFAULT_TIMEOUT}s)")
    parser.add_argument("--delay", type=float, default=DEFAULT_DELAY, help=f"Request delay (default: {DEFAULT_DELAY}s)")
    parser.add_argument("--max-requests", type=int, default=DEFAULT_MAX_REQUESTS, help=f"Max requests (default: {DEFAULT_MAX_REQUESTS})")
    parser.add_argument("--threads", type=int, default=DEFAULT_THREADS, help=f"Thread count (default: {DEFAULT_THREADS})")
    parser.add_argument("--out", "-o", default=DEFAULT_LOG_FILE, help="Output file")
    parser.add_argument("--export-format", choices=["json", "txt", "html", "all"], help="Export report format")
    parser.add_argument("--verbose", "-v", action="store_true", help="Verbose logging")
    parser.add_argument("--quiet", "-q", action="store_true", help="Minimal output")
    
    return parser


def main() -> int:
    parser = build_parser()
    args = parser.parse_args()
    
    # Configure logging
    if args.quiet:
        log_level = logging.WARNING
    elif args.verbose:
        log_level = logging.DEBUG
    else:
        log_level = logging.INFO
    
    logging.basicConfig(
        level=log_level,
        format="%(asctime)s | %(levelname)-8s | %(message)s",
        datefmt="%H:%M:%S"
    )
    
    # If no target specified, run interactive mode
    if not args.target:
        run_interactive()
        return 0
    
    # Command line mode
    print(BANNER)
    
    # Determine modules
    if args.full_scan:
        enable_sqli = enable_xss = enable_cmdi = enable_lfi = True
        enable_cors = enable_cookies = enable_tech = True
        enable_headers = enable_exposure = True
        time_based = True
        max_requests = 5000
        threads = 15
    elif args.sqli_only:
        enable_sqli = True
        enable_xss = enable_cmdi = enable_lfi = enable_cors = False
        enable_cookies = enable_tech = enable_headers = enable_exposure = False
        time_based = args.time_based
        max_requests = args.max_requests
        threads = args.threads
    elif args.xss_only:
        enable_xss = True
        enable_sqli = enable_cmdi = enable_lfi = enable_cors = False
        enable_cookies = enable_tech = enable_headers = enable_exposure = False
        time_based = False
        max_requests = args.max_requests
        threads = args.threads
    elif args.headers_only:
        enable_headers = True
        enable_sqli = enable_xss = enable_cmdi = enable_lfi = False
        enable_cors = enable_cookies = enable_tech = enable_exposure = False
        time_based = False
        max_requests = 100
        threads = 1
    else:
        enable_sqli = not args.no_sqli
        enable_xss = not args.no_xss
        enable_cmdi = not args.no_cmdi
        enable_lfi = not args.no_lfi
        enable_cors = not args.no_cors
        enable_cookies = not args.no_cookies
        enable_tech = not args.no_tech
        enable_headers = not args.no_headers
        enable_exposure = not args.no_exposure
        time_based = args.time_based
        max_requests = args.max_requests
        threads = args.threads
    
    agent = PentestAgent(
        args.target,
        verify_tls=args.verify_tls,
        timeout_s=args.timeout,
        delay_s=args.delay,
        max_requests=max_requests,
        max_depth=args.max_depth,
        out_file=args.out,
        enable_sqli=enable_sqli,
        enable_xss=enable_xss,
        enable_cmdi=enable_cmdi,
        enable_lfi=enable_lfi,
        enable_cors=enable_cors,
        enable_cookies=enable_cookies,
        enable_tech=enable_tech,
        enable_headers=enable_headers,
        enable_exposure=enable_exposure,
        time_based_sqli=time_based,
        verbose=args.verbose,
        threads=threads,
    )
    
    try:
        agent.scan(crawl=not args.no_crawl)
    except KeyboardInterrupt:
        logging.warning("Scan interrupted by user")
    
    agent.print_summary()
    
    # Export report if requested
    if args.export_format:
        if args.export_format == 'json':
            agent.export_json()
        elif args.export_format == 'txt':
            agent.export_txt()
        elif args.export_format == 'html':
            agent.export_html()
        elif args.export_format == 'all':
            agent.export_json()
            agent.export_txt()
            agent.export_html()
    
    return 1 if any(f.severity in ("CRITICAL", "HIGH") for f in agent._findings) else 0


if __name__ == "__main__":
    raise SystemExit(main())